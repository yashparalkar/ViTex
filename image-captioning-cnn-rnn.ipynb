{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072},{"sourceId":12852166,"sourceType":"datasetVersion","datasetId":8128837},{"sourceId":12852268,"sourceType":"datasetVersion","datasetId":8128922}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir data\n!mkdir models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-24T10:23:00.327825Z","iopub.execute_input":"2025-08-24T10:23:00.328050Z","iopub.status.idle":"2025-08-24T10:23:00.560141Z","shell.execute_reply.started":"2025-08-24T10:23:00.328032Z","shell.execute_reply":"2025-08-24T10:23:00.559432Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet152(pretrained=True)\n        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n        self.resnet = nn.Sequential(*modules)\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n        \n    def forward(self, images):\n        \"\"\"Extract feature vectors from input images.\"\"\"\n        with torch.no_grad():\n            features = self.resnet(images)\n        features = features.reshape(features.size(0), -1)\n        features = self.bn(self.linear(features))\n        return features\n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.max_seg_length = max_seq_length\n        \n    def forward(self, features, captions, lengths):\n        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n        embeddings = self.embed(captions)\n        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n        hiddens, _ = self.lstm(packed)\n        outputs = self.linear(hiddens[0])\n        return outputs\n    \n    def sample(self, features, states=None):\n        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n        sampled_ids = []\n        inputs = features.unsqueeze(1)\n        for i in range(self.max_seg_length):\n            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n            sampled_ids.append(predicted)\n            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n        return sampled_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T10:24:13.493504Z","iopub.execute_input":"2025-08-24T10:24:13.494148Z","iopub.status.idle":"2025-08-24T10:24:13.503973Z","shell.execute_reply.started":"2025-08-24T10:24:13.494126Z","shell.execute_reply":"2025-08-24T10:24:13.503225Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Vocabulary(object):\n    \"\"\"Simple vocabulary wrapper.\"\"\"\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T10:24:21.889944Z","iopub.execute_input":"2025-08-24T10:24:21.890250Z","iopub.status.idle":"2025-08-24T10:24:21.895416Z","shell.execute_reply.started":"2025-08-24T10:24:21.890229Z","shell.execute_reply":"2025-08-24T10:24:21.894700Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport os\nimport pickle\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torchvision import transforms\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef main():\n    # Hyperparameters & paths\n    model_path = '/kaggle/working/models/'\n    crop_size = 224\n    vocab_path = '/kaggle/input/tokenized-vocab/vocab.pkl'\n    image_dir = '/kaggle/input/resized-imagescoco/resized2014'\n    caption_path = '/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/captions_train2014.json'\n    log_step = 10\n    save_step = 1000\n    \n    embed_size = 256\n    hidden_size = 512\n    num_layers = 1\n    \n    num_epochs = 5\n    batch_size = 128\n    num_workers = 2\n    learning_rate = 0.001\n    \n    # Create model directory\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n    \n    # Image preprocessing\n    transform = transforms.Compose([ \n        transforms.RandomCrop(crop_size),\n        transforms.RandomHorizontalFlip(), \n        transforms.ToTensor(), \n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load vocabulary\n    with open(vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n    \n    # Build data loader\n    data_loader = get_loader(image_dir, caption_path, vocab, \n                             transform, batch_size,\n                             shuffle=True, num_workers=num_workers) \n\n    # Build the models\n    encoder = EncoderCNN(embed_size).to(device)\n    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n    optimizer = torch.optim.Adam(params, lr=learning_rate)\n    \n    # Training loop\n    total_step = len(data_loader)\n    for epoch in range(num_epochs):\n        for i, (images, captions, lengths) in enumerate(data_loader):\n            \n            images = images.to(device)\n            captions = captions.to(device)\n            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n            \n            # Forward, backward and optimize\n            features = encoder(images)\n            outputs = decoder(features, captions, lengths)\n            loss = criterion(outputs, targets)\n            decoder.zero_grad()\n            encoder.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Logging\n            if i % log_step == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n                      .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n                \n            # Save checkpoints\n            if (i+1) % save_step == 0:\n                torch.save(decoder.state_dict(), os.path.join(\n                    model_path, f'decoder-{epoch+1}-{i+1}.ckpt'))\n                torch.save(encoder.state_dict(), os.path.join(\n                    model_path, f'encoder-{epoch+1}-{i+1}.ckpt'))\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-08-24T12:36:20.555526Z","shell.execute_reply.started":"2025-08-24T10:24:28.043257Z","shell.execute_reply":"2025-08-24T12:36:20.554551Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Epoch [1/5], Step [80/3236], Loss: 2.1820, Perplexity: 8.8639\nEpoch [1/5], Step [90/3236], Loss: 2.2243, Perplexity: 9.2473\nEpoch [1/5], Step [100/3236], Loss: 2.1687, Perplexity: 8.7466\nEpoch [1/5], Step [110/3236], Loss: 2.0501, Perplexity: 7.7687\nEpoch [1/5], Step [120/3236], Loss: 2.1077, Perplexity: 8.2289\nEpoch [1/5], Step [130/3236], Loss: 2.1296, Perplexity: 8.4117\nEpoch [1/5], Step [140/3236], Loss: 2.1856, Perplexity: 8.8957\nEpoch [1/5], Step [150/3236], Loss: 2.1648, Perplexity: 8.7125\nEpoch [1/5], Step [160/3236], Loss: 2.0680, Perplexity: 7.9091\nEpoch [1/5], Step [170/3236], Loss: 2.1382, Perplexity: 8.4840\nEpoch [1/5], Step [180/3236], Loss: 2.2571, Perplexity: 9.5550\nEpoch [1/5], Step [190/3236], Loss: 2.1295, Perplexity: 8.4107\nEpoch [1/5], Step [200/3236], Loss: 2.2033, Perplexity: 9.0551\nEpoch [1/5], Step [210/3236], Loss: 2.2001, Perplexity: 9.0261\nEpoch [1/5], Step [220/3236], Loss: 2.1454, Perplexity: 8.5452\nEpoch [1/5], Step [230/3236], Loss: 2.1323, Perplexity: 8.4339\nEpoch [1/5], Step [240/3236], Loss: 2.1741, Perplexity: 8.7946\nEpoch [1/5], Step [250/3236], Loss: 2.1526, Perplexity: 8.6069\nEpoch [1/5], Step [260/3236], Loss: 2.1504, Perplexity: 8.5885\nEpoch [1/5], Step [270/3236], Loss: 2.1734, Perplexity: 8.7882\nEpoch [1/5], Step [280/3236], Loss: 2.1768, Perplexity: 8.8178\nEpoch [1/5], Step [290/3236], Loss: 2.1200, Perplexity: 8.3313\nEpoch [1/5], Step [300/3236], Loss: 2.0685, Perplexity: 7.9133\nEpoch [1/5], Step [310/3236], Loss: 2.1960, Perplexity: 8.9889\nEpoch [1/5], Step [320/3236], Loss: 2.1938, Perplexity: 8.9696\nEpoch [1/5], Step [330/3236], Loss: 2.1521, Perplexity: 8.6028\nEpoch [1/5], Step [340/3236], Loss: 2.1670, Perplexity: 8.7323\nEpoch [1/5], Step [350/3236], Loss: 2.2089, Perplexity: 9.1060\nEpoch [1/5], Step [360/3236], Loss: 1.9838, Perplexity: 7.2702\nEpoch [1/5], Step [370/3236], Loss: 2.1485, Perplexity: 8.5720\nEpoch [1/5], Step [380/3236], Loss: 2.2526, Perplexity: 9.5127\nEpoch [1/5], Step [390/3236], Loss: 2.1446, Perplexity: 8.5390\nEpoch [1/5], Step [400/3236], Loss: 2.1087, Perplexity: 8.2374\nEpoch [1/5], Step [410/3236], Loss: 2.0963, Perplexity: 8.1359\nEpoch [1/5], Step [420/3236], Loss: 2.2254, Perplexity: 9.2568\nEpoch [1/5], Step [430/3236], Loss: 1.9750, Perplexity: 7.2065\nEpoch [1/5], Step [440/3236], Loss: 2.1337, Perplexity: 8.4462\nEpoch [1/5], Step [450/3236], Loss: 2.1357, Perplexity: 8.4628\nEpoch [1/5], Step [460/3236], Loss: 2.2220, Perplexity: 9.2258\nEpoch [1/5], Step [470/3236], Loss: 2.2176, Perplexity: 9.1852\nEpoch [1/5], Step [480/3236], Loss: 2.1985, Perplexity: 9.0112\nEpoch [1/5], Step [490/3236], Loss: 2.3175, Perplexity: 10.1498\nEpoch [1/5], Step [500/3236], Loss: 2.1511, Perplexity: 8.5940\nEpoch [1/5], Step [510/3236], Loss: 2.3222, Perplexity: 10.1984\nEpoch [1/5], Step [520/3236], Loss: 2.1903, Perplexity: 8.9377\nEpoch [1/5], Step [530/3236], Loss: 2.3125, Perplexity: 10.0997\nEpoch [1/5], Step [540/3236], Loss: 2.2064, Perplexity: 9.0833\nEpoch [1/5], Step [550/3236], Loss: 2.0948, Perplexity: 8.1241\nEpoch [1/5], Step [560/3236], Loss: 2.2374, Perplexity: 9.3686\nEpoch [1/5], Step [570/3236], Loss: 2.0757, Perplexity: 7.9703\nEpoch [1/5], Step [580/3236], Loss: 2.2508, Perplexity: 9.4955\nEpoch [1/5], Step [590/3236], Loss: 2.0994, Perplexity: 8.1614\nEpoch [1/5], Step [600/3236], Loss: 2.1255, Perplexity: 8.3772\nEpoch [1/5], Step [610/3236], Loss: 2.2103, Perplexity: 9.1185\nEpoch [1/5], Step [620/3236], Loss: 2.1099, Perplexity: 8.2477\nEpoch [1/5], Step [630/3236], Loss: 2.1612, Perplexity: 8.6818\nEpoch [1/5], Step [640/3236], Loss: 2.1748, Perplexity: 8.8005\nEpoch [1/5], Step [650/3236], Loss: 2.1082, Perplexity: 8.2337\nEpoch [1/5], Step [660/3236], Loss: 2.2232, Perplexity: 9.2365\nEpoch [1/5], Step [670/3236], Loss: 2.2578, Perplexity: 9.5616\nEpoch [1/5], Step [680/3236], Loss: 2.2775, Perplexity: 9.7519\nEpoch [1/5], Step [690/3236], Loss: 2.2541, Perplexity: 9.5270\nEpoch [1/5], Step [700/3236], Loss: 2.0419, Perplexity: 7.7052\nEpoch [1/5], Step [710/3236], Loss: 2.1187, Perplexity: 8.3200\nEpoch [1/5], Step [720/3236], Loss: 2.1916, Perplexity: 8.9491\nEpoch [1/5], Step [730/3236], Loss: 2.2069, Perplexity: 9.0875\nEpoch [1/5], Step [740/3236], Loss: 2.1274, Perplexity: 8.3929\nEpoch [1/5], Step [750/3236], Loss: 2.1549, Perplexity: 8.6270\nEpoch [1/5], Step [760/3236], Loss: 2.1364, Perplexity: 8.4689\nEpoch [1/5], Step [770/3236], Loss: 2.1871, Perplexity: 8.9097\nEpoch [1/5], Step [780/3236], Loss: 2.2035, Perplexity: 9.0569\nEpoch [1/5], Step [790/3236], Loss: 2.0202, Perplexity: 7.5402\nEpoch [1/5], Step [800/3236], Loss: 2.3184, Perplexity: 10.1590\nEpoch [1/5], Step [810/3236], Loss: 2.1661, Perplexity: 8.7239\nEpoch [1/5], Step [820/3236], Loss: 2.0587, Perplexity: 7.8361\nEpoch [1/5], Step [830/3236], Loss: 2.1658, Perplexity: 8.7219\nEpoch [1/5], Step [840/3236], Loss: 2.1189, Perplexity: 8.3223\nEpoch [1/5], Step [850/3236], Loss: 2.1269, Perplexity: 8.3890\nEpoch [1/5], Step [860/3236], Loss: 2.1970, Perplexity: 8.9977\nEpoch [1/5], Step [870/3236], Loss: 2.2513, Perplexity: 9.5004\nEpoch [1/5], Step [880/3236], Loss: 2.3086, Perplexity: 10.0608\nEpoch [1/5], Step [890/3236], Loss: 2.1806, Perplexity: 8.8518\nEpoch [1/5], Step [900/3236], Loss: 2.2547, Perplexity: 9.5328\nEpoch [1/5], Step [910/3236], Loss: 2.1886, Perplexity: 8.9228\nEpoch [1/5], Step [920/3236], Loss: 2.1424, Perplexity: 8.5200\nEpoch [1/5], Step [930/3236], Loss: 2.1837, Perplexity: 8.8789\nEpoch [1/5], Step [940/3236], Loss: 2.1454, Perplexity: 8.5454\nEpoch [1/5], Step [950/3236], Loss: 2.1149, Perplexity: 8.2886\nEpoch [1/5], Step [960/3236], Loss: 2.1790, Perplexity: 8.8379\nEpoch [1/5], Step [970/3236], Loss: 2.1710, Perplexity: 8.7669\nEpoch [1/5], Step [980/3236], Loss: 1.9999, Perplexity: 7.3886\nEpoch [1/5], Step [990/3236], Loss: 2.1571, Perplexity: 8.6457\nEpoch [1/5], Step [1000/3236], Loss: 1.9927, Perplexity: 7.3351\nEpoch [1/5], Step [1010/3236], Loss: 2.1357, Perplexity: 8.4629\nEpoch [1/5], Step [1020/3236], Loss: 2.1444, Perplexity: 8.5372\nEpoch [1/5], Step [1030/3236], Loss: 2.2302, Perplexity: 9.3018\nEpoch [1/5], Step [1040/3236], Loss: 2.2311, Perplexity: 9.3100\nEpoch [1/5], Step [1050/3236], Loss: 2.0145, Perplexity: 7.4972\nEpoch [1/5], Step [1060/3236], Loss: 2.0254, Perplexity: 7.5790\nEpoch [1/5], Step [1070/3236], Loss: 2.0810, Perplexity: 8.0125\nEpoch [1/5], Step [1080/3236], Loss: 2.1082, Perplexity: 8.2332\nEpoch [1/5], Step [1090/3236], Loss: 2.1324, Perplexity: 8.4348\nEpoch [1/5], Step [1100/3236], Loss: 2.1074, Perplexity: 8.2271\nEpoch [1/5], Step [1110/3236], Loss: 2.0798, Perplexity: 8.0032\nEpoch [1/5], Step [1120/3236], Loss: 2.0982, Perplexity: 8.1517\nEpoch [1/5], Step [1130/3236], Loss: 2.1976, Perplexity: 9.0032\nEpoch [1/5], Step [1140/3236], Loss: 2.1471, Perplexity: 8.5598\nEpoch [1/5], Step [1150/3236], Loss: 2.0380, Perplexity: 7.6756\nEpoch [1/5], Step [1160/3236], Loss: 2.1734, Perplexity: 8.7878\nEpoch [1/5], Step [1170/3236], Loss: 2.1263, Perplexity: 8.3835\nEpoch [1/5], Step [1180/3236], Loss: 2.2194, Perplexity: 9.2022\nEpoch [1/5], Step [1190/3236], Loss: 2.0992, Perplexity: 8.1593\nEpoch [1/5], Step [1200/3236], Loss: 2.0251, Perplexity: 7.5769\nEpoch [1/5], Step [1210/3236], Loss: 2.1071, Perplexity: 8.2242\nEpoch [1/5], Step [1220/3236], Loss: 2.1200, Perplexity: 8.3309\nEpoch [1/5], Step [1230/3236], Loss: 2.2220, Perplexity: 9.2255\nEpoch [1/5], Step [1240/3236], Loss: 2.1583, Perplexity: 8.6564\nEpoch [1/5], Step [1250/3236], Loss: 2.2009, Perplexity: 9.0329\nEpoch [1/5], Step [1260/3236], Loss: 2.1206, Perplexity: 8.3364\nEpoch [1/5], Step [1270/3236], Loss: 2.1424, Perplexity: 8.5202\nEpoch [1/5], Step [1280/3236], Loss: 2.1257, Perplexity: 8.3788\nEpoch [1/5], Step [1290/3236], Loss: 2.1222, Perplexity: 8.3494\nEpoch [1/5], Step [1300/3236], Loss: 2.1452, Perplexity: 8.5440\nEpoch [1/5], Step [1310/3236], Loss: 2.2963, Perplexity: 9.9369\nEpoch [1/5], Step [1320/3236], Loss: 2.0161, Perplexity: 7.5087\nEpoch [1/5], Step [1330/3236], Loss: 2.2617, Perplexity: 9.5995\nEpoch [1/5], Step [1340/3236], Loss: 2.1572, Perplexity: 8.6473\nEpoch [1/5], Step [1350/3236], Loss: 2.1122, Perplexity: 8.2666\nEpoch [1/5], Step [1360/3236], Loss: 2.1119, Perplexity: 8.2642\nEpoch [1/5], Step [1370/3236], Loss: 2.1695, Perplexity: 8.7539\nEpoch [1/5], Step [1380/3236], Loss: 2.1809, Perplexity: 8.8545\nEpoch [1/5], Step [1390/3236], Loss: 2.1102, Perplexity: 8.2497\nEpoch [1/5], Step [1400/3236], Loss: 2.1686, Perplexity: 8.7460\nEpoch [1/5], Step [1410/3236], Loss: 2.1737, Perplexity: 8.7909\nEpoch [1/5], Step [1420/3236], Loss: 2.1119, Perplexity: 8.2641\nEpoch [1/5], Step [1430/3236], Loss: 2.2462, Perplexity: 9.4521\nEpoch [1/5], Step [1440/3236], Loss: 2.1484, Perplexity: 8.5711\nEpoch [1/5], Step [1450/3236], Loss: 2.1492, Perplexity: 8.5781\nEpoch [1/5], Step [1460/3236], Loss: 2.1724, Perplexity: 8.7789\nEpoch [1/5], Step [1470/3236], Loss: 2.2047, Perplexity: 9.0677\nEpoch [1/5], Step [1480/3236], Loss: 2.1226, Perplexity: 8.3529\nEpoch [1/5], Step [1490/3236], Loss: 2.1103, Perplexity: 8.2504\nEpoch [1/5], Step [1500/3236], Loss: 2.1979, Perplexity: 9.0060\nEpoch [1/5], Step [1510/3236], Loss: 2.0342, Perplexity: 7.6464\nEpoch [1/5], Step [1520/3236], Loss: 2.0289, Perplexity: 7.6055\nEpoch [1/5], Step [1530/3236], Loss: 2.1848, Perplexity: 8.8891\nEpoch [1/5], Step [1540/3236], Loss: 2.1877, Perplexity: 8.9145\nEpoch [1/5], Step [1550/3236], Loss: 2.0307, Perplexity: 7.6193\nEpoch [1/5], Step [1560/3236], Loss: 2.1371, Perplexity: 8.4750\nEpoch [1/5], Step [1570/3236], Loss: 2.1678, Perplexity: 8.7388\nEpoch [1/5], Step [1580/3236], Loss: 1.9718, Perplexity: 7.1837\nEpoch [1/5], Step [1590/3236], Loss: 2.1715, Perplexity: 8.7713\nEpoch [1/5], Step [1600/3236], Loss: 2.1252, Perplexity: 8.3747\nEpoch [1/5], Step [1610/3236], Loss: 1.9860, Perplexity: 7.2866\nEpoch [1/5], Step [1620/3236], Loss: 2.2433, Perplexity: 9.4240\nEpoch [1/5], Step [1630/3236], Loss: 2.1463, Perplexity: 8.5530\nEpoch [1/5], Step [1640/3236], Loss: 2.1121, Perplexity: 8.2653\nEpoch [1/5], Step [1650/3236], Loss: 2.0449, Perplexity: 7.7287\nEpoch [1/5], Step [1660/3236], Loss: 2.1165, Perplexity: 8.3024\nEpoch [1/5], Step [1670/3236], Loss: 2.1253, Perplexity: 8.3754\nEpoch [1/5], Step [1680/3236], Loss: 2.0946, Perplexity: 8.1225\nEpoch [1/5], Step [1690/3236], Loss: 1.9888, Perplexity: 7.3069\nEpoch [1/5], Step [1700/3236], Loss: 2.0625, Perplexity: 7.8654\nEpoch [1/5], Step [1710/3236], Loss: 2.3244, Perplexity: 10.2203\nEpoch [1/5], Step [1720/3236], Loss: 2.2213, Perplexity: 9.2197\nEpoch [1/5], Step [1730/3236], Loss: 2.1523, Perplexity: 8.6042\nEpoch [1/5], Step [1740/3236], Loss: 2.0617, Perplexity: 7.8595\nEpoch [1/5], Step [1750/3236], Loss: 2.1289, Perplexity: 8.4052\nEpoch [1/5], Step [1760/3236], Loss: 2.1890, Perplexity: 8.9261\nEpoch [1/5], Step [1770/3236], Loss: 1.9350, Perplexity: 6.9242\nEpoch [1/5], Step [1780/3236], Loss: 2.0667, Perplexity: 7.8989\nEpoch [1/5], Step [1790/3236], Loss: 2.2029, Perplexity: 9.0515\nEpoch [1/5], Step [1800/3236], Loss: 2.2393, Perplexity: 9.3864\nEpoch [1/5], Step [1810/3236], Loss: 2.0173, Perplexity: 7.5181\nEpoch [1/5], Step [1820/3236], Loss: 2.2266, Perplexity: 9.2682\nEpoch [1/5], Step [1830/3236], Loss: 2.3073, Perplexity: 10.0468\nEpoch [1/5], Step [1840/3236], Loss: 2.2299, Perplexity: 9.2989\nEpoch [1/5], Step [1850/3236], Loss: 2.1741, Perplexity: 8.7943\nEpoch [1/5], Step [1860/3236], Loss: 2.2029, Perplexity: 9.0512\nEpoch [1/5], Step [1870/3236], Loss: 2.1447, Perplexity: 8.5395\nEpoch [1/5], Step [1880/3236], Loss: 2.1324, Perplexity: 8.4353\nEpoch [1/5], Step [1890/3236], Loss: 2.1016, Perplexity: 8.1791\nEpoch [1/5], Step [1900/3236], Loss: 2.2315, Perplexity: 9.3142\nEpoch [1/5], Step [1910/3236], Loss: 2.0561, Perplexity: 7.8152\nEpoch [1/5], Step [1920/3236], Loss: 2.0348, Perplexity: 7.6509\nEpoch [1/5], Step [1930/3236], Loss: 2.1548, Perplexity: 8.6258\nEpoch [1/5], Step [1940/3236], Loss: 1.9967, Perplexity: 7.3646\nEpoch [1/5], Step [1950/3236], Loss: 1.9774, Perplexity: 7.2240\nEpoch [1/5], Step [1960/3236], Loss: 2.1388, Perplexity: 8.4895\nEpoch [1/5], Step [1970/3236], Loss: 2.1505, Perplexity: 8.5893\nEpoch [1/5], Step [1980/3236], Loss: 2.0610, Perplexity: 7.8537\nEpoch [1/5], Step [1990/3236], Loss: 2.0348, Perplexity: 7.6507\nEpoch [1/5], Step [2000/3236], Loss: 2.3376, Perplexity: 10.3561\nEpoch [1/5], Step [2010/3236], Loss: 2.0637, Perplexity: 7.8751\nEpoch [1/5], Step [2020/3236], Loss: 2.0263, Perplexity: 7.5862\nEpoch [1/5], Step [2030/3236], Loss: 2.1455, Perplexity: 8.5460\nEpoch [1/5], Step [2040/3236], Loss: 2.1221, Perplexity: 8.3486\nEpoch [1/5], Step [2050/3236], Loss: 2.1873, Perplexity: 8.9115\nEpoch [1/5], Step [2060/3236], Loss: 1.9150, Perplexity: 6.7873\nEpoch [1/5], Step [2070/3236], Loss: 2.2580, Perplexity: 9.5641\nEpoch [1/5], Step [2080/3236], Loss: 2.0491, Perplexity: 7.7612\nEpoch [1/5], Step [2090/3236], Loss: 2.1455, Perplexity: 8.5462\nEpoch [1/5], Step [2100/3236], Loss: 2.2152, Perplexity: 9.1635\nEpoch [1/5], Step [2110/3236], Loss: 2.2198, Perplexity: 9.2059\nEpoch [1/5], Step [2120/3236], Loss: 2.0994, Perplexity: 8.1616\nEpoch [1/5], Step [2130/3236], Loss: 2.0711, Perplexity: 7.9333\nEpoch [1/5], Step [2140/3236], Loss: 2.0642, Perplexity: 7.8793\nEpoch [1/5], Step [2150/3236], Loss: 2.0214, Perplexity: 7.5490\nEpoch [1/5], Step [2160/3236], Loss: 2.1733, Perplexity: 8.7876\nEpoch [1/5], Step [2170/3236], Loss: 1.9934, Perplexity: 7.3404\nEpoch [1/5], Step [2180/3236], Loss: 2.1942, Perplexity: 8.9725\nEpoch [1/5], Step [2190/3236], Loss: 2.1878, Perplexity: 8.9152\nEpoch [1/5], Step [2200/3236], Loss: 2.1936, Perplexity: 8.9676\nEpoch [1/5], Step [2210/3236], Loss: 2.2242, Perplexity: 9.2463\nEpoch [1/5], Step [2220/3236], Loss: 1.9674, Perplexity: 7.1518\nEpoch [1/5], Step [2230/3236], Loss: 2.0402, Perplexity: 7.6918\nEpoch [1/5], Step [2240/3236], Loss: 2.0837, Perplexity: 8.0343\nEpoch [1/5], Step [2250/3236], Loss: 2.1928, Perplexity: 8.9603\nEpoch [1/5], Step [2260/3236], Loss: 2.3425, Perplexity: 10.4072\nEpoch [1/5], Step [2270/3236], Loss: 2.1488, Perplexity: 8.5748\nEpoch [1/5], Step [2280/3236], Loss: 2.1000, Perplexity: 8.1664\nEpoch [1/5], Step [2290/3236], Loss: 2.0549, Perplexity: 7.8060\nEpoch [1/5], Step [2300/3236], Loss: 2.0530, Perplexity: 7.7916\nEpoch [1/5], Step [2310/3236], Loss: 2.1622, Perplexity: 8.6904\nEpoch [1/5], Step [2320/3236], Loss: 2.1395, Perplexity: 8.4953\nEpoch [1/5], Step [2330/3236], Loss: 2.1702, Perplexity: 8.7599\nEpoch [1/5], Step [2340/3236], Loss: 2.1788, Perplexity: 8.8360\nEpoch [1/5], Step [2350/3236], Loss: 2.0170, Perplexity: 7.5155\nEpoch [1/5], Step [2360/3236], Loss: 2.0058, Perplexity: 7.4322\nEpoch [1/5], Step [2370/3236], Loss: 2.1815, Perplexity: 8.8592\nEpoch [1/5], Step [2380/3236], Loss: 1.9963, Perplexity: 7.3616\nEpoch [1/5], Step [2390/3236], Loss: 2.0019, Perplexity: 7.4031\nEpoch [1/5], Step [2400/3236], Loss: 2.1984, Perplexity: 9.0109\nEpoch [1/5], Step [2410/3236], Loss: 2.0726, Perplexity: 7.9453\nEpoch [1/5], Step [2420/3236], Loss: 2.0602, Perplexity: 7.8478\nEpoch [1/5], Step [2430/3236], Loss: 2.2093, Perplexity: 9.1097\nEpoch [1/5], Step [2440/3236], Loss: 2.2449, Perplexity: 9.4394\nEpoch [1/5], Step [2450/3236], Loss: 2.1888, Perplexity: 8.9245\nEpoch [1/5], Step [2460/3236], Loss: 2.0982, Perplexity: 8.1517\nEpoch [1/5], Step [2470/3236], Loss: 2.1690, Perplexity: 8.7499\nEpoch [1/5], Step [2480/3236], Loss: 2.0452, Perplexity: 7.7306\nEpoch [1/5], Step [2490/3236], Loss: 2.0018, Perplexity: 7.4027\nEpoch [1/5], Step [2500/3236], Loss: 2.1247, Perplexity: 8.3706\nEpoch [1/5], Step [2510/3236], Loss: 2.0662, Perplexity: 7.8947\nEpoch [1/5], Step [2520/3236], Loss: 2.1438, Perplexity: 8.5321\nEpoch [1/5], Step [2530/3236], Loss: 2.1264, Perplexity: 8.3847\nEpoch [1/5], Step [2540/3236], Loss: 2.1044, Perplexity: 8.2022\nEpoch [1/5], Step [2550/3236], Loss: 2.1328, Perplexity: 8.4383\nEpoch [1/5], Step [2560/3236], Loss: 2.1163, Perplexity: 8.3007\nEpoch [1/5], Step [2570/3236], Loss: 2.1063, Perplexity: 8.2175\nEpoch [1/5], Step [2580/3236], Loss: 2.0477, Perplexity: 7.7499\nEpoch [1/5], Step [2590/3236], Loss: 2.0520, Perplexity: 7.7835\nEpoch [1/5], Step [2600/3236], Loss: 2.0206, Perplexity: 7.5432\nEpoch [1/5], Step [2610/3236], Loss: 2.2032, Perplexity: 9.0541\nEpoch [1/5], Step [2620/3236], Loss: 2.0869, Perplexity: 8.0602\nEpoch [1/5], Step [2630/3236], Loss: 2.0706, Perplexity: 7.9297\nEpoch [1/5], Step [2640/3236], Loss: 2.2431, Perplexity: 9.4229\nEpoch [1/5], Step [2650/3236], Loss: 2.1983, Perplexity: 9.0097\nEpoch [1/5], Step [2660/3236], Loss: 2.0535, Perplexity: 7.7951\nEpoch [1/5], Step [2670/3236], Loss: 2.1616, Perplexity: 8.6853\nEpoch [1/5], Step [2680/3236], Loss: 2.0348, Perplexity: 7.6510\nEpoch [1/5], Step [2690/3236], Loss: 2.1163, Perplexity: 8.3001\nEpoch [1/5], Step [2700/3236], Loss: 2.0819, Perplexity: 8.0201\nEpoch [1/5], Step [2710/3236], Loss: 2.0401, Perplexity: 7.6916\nEpoch [1/5], Step [2720/3236], Loss: 2.1049, Perplexity: 8.2066\nEpoch [1/5], Step [2730/3236], Loss: 2.0302, Perplexity: 7.6158\nEpoch [1/5], Step [2740/3236], Loss: 2.1145, Perplexity: 8.2851\nEpoch [1/5], Step [2750/3236], Loss: 2.0589, Perplexity: 7.8377\nEpoch [1/5], Step [2760/3236], Loss: 2.1651, Perplexity: 8.7155\nEpoch [1/5], Step [2770/3236], Loss: 1.9818, Perplexity: 7.2558\nEpoch [1/5], Step [2780/3236], Loss: 2.2327, Perplexity: 9.3254\nEpoch [1/5], Step [2790/3236], Loss: 2.1047, Perplexity: 8.2045\nEpoch [1/5], Step [2800/3236], Loss: 2.1271, Perplexity: 8.3902\nEpoch [1/5], Step [2810/3236], Loss: 2.1518, Perplexity: 8.6004\nEpoch [1/5], Step [2820/3236], Loss: 2.2460, Perplexity: 9.4497\nEpoch [1/5], Step [2830/3236], Loss: 2.0533, Perplexity: 7.7937\nEpoch [1/5], Step [2840/3236], Loss: 2.0949, Perplexity: 8.1244\nEpoch [1/5], Step [2850/3236], Loss: 2.1840, Perplexity: 8.8817\nEpoch [1/5], Step [2860/3236], Loss: 2.1259, Perplexity: 8.3801\nEpoch [1/5], Step [2870/3236], Loss: 2.0766, Perplexity: 7.9772\nEpoch [1/5], Step [2880/3236], Loss: 2.0402, Perplexity: 7.6921\nEpoch [1/5], Step [2890/3236], Loss: 2.0389, Perplexity: 7.6823\nEpoch [1/5], Step [2900/3236], Loss: 2.1232, Perplexity: 8.3580\nEpoch [1/5], Step [2910/3236], Loss: 1.9272, Perplexity: 6.8705\nEpoch [1/5], Step [2920/3236], Loss: 1.8922, Perplexity: 6.6336\nEpoch [1/5], Step [2930/3236], Loss: 1.9797, Perplexity: 7.2409\nEpoch [1/5], Step [2940/3236], Loss: 2.1823, Perplexity: 8.8669\nEpoch [1/5], Step [2950/3236], Loss: 2.1464, Perplexity: 8.5539\nEpoch [1/5], Step [2960/3236], Loss: 2.1687, Perplexity: 8.7469\nEpoch [1/5], Step [2970/3236], Loss: 2.1139, Perplexity: 8.2804\nEpoch [1/5], Step [2980/3236], Loss: 2.1023, Perplexity: 8.1854\nEpoch [1/5], Step [2990/3236], Loss: 1.9998, Perplexity: 7.3879\nEpoch [1/5], Step [3000/3236], Loss: 2.0779, Perplexity: 7.9881\nEpoch [1/5], Step [3010/3236], Loss: 2.0909, Perplexity: 8.0925\nEpoch [1/5], Step [3020/3236], Loss: 2.1042, Perplexity: 8.2008\nEpoch [1/5], Step [3030/3236], Loss: 2.0504, Perplexity: 7.7707\nEpoch [1/5], Step [3040/3236], Loss: 2.1696, Perplexity: 8.7550\nEpoch [1/5], Step [3050/3236], Loss: 2.1560, Perplexity: 8.6361\nEpoch [1/5], Step [3060/3236], Loss: 2.0807, Perplexity: 8.0101\nEpoch [1/5], Step [3070/3236], Loss: 2.2004, Perplexity: 9.0289\nEpoch [1/5], Step [3080/3236], Loss: 2.2509, Perplexity: 9.4962\nEpoch [1/5], Step [3090/3236], Loss: 2.0134, Perplexity: 7.4889\nEpoch [1/5], Step [3100/3236], Loss: 2.0683, Perplexity: 7.9112\nEpoch [1/5], Step [3110/3236], Loss: 2.0168, Perplexity: 7.5139\nEpoch [1/5], Step [3120/3236], Loss: 2.1049, Perplexity: 8.2063\nEpoch [1/5], Step [3130/3236], Loss: 2.1185, Perplexity: 8.3189\nEpoch [1/5], Step [3140/3236], Loss: 2.1224, Perplexity: 8.3511\nEpoch [1/5], Step [3150/3236], Loss: 2.0335, Perplexity: 7.6405\nEpoch [1/5], Step [3160/3236], Loss: 2.1075, Perplexity: 8.2276\nEpoch [1/5], Step [3170/3236], Loss: 1.9689, Perplexity: 7.1630\nEpoch [1/5], Step [3180/3236], Loss: 2.1479, Perplexity: 8.5670\nEpoch [1/5], Step [3190/3236], Loss: 2.0038, Perplexity: 7.4172\nEpoch [1/5], Step [3200/3236], Loss: 2.0113, Perplexity: 7.4733\nEpoch [1/5], Step [3210/3236], Loss: 2.0934, Perplexity: 8.1123\nEpoch [1/5], Step [3220/3236], Loss: 1.9650, Perplexity: 7.1346\nEpoch [1/5], Step [3230/3236], Loss: 2.2002, Perplexity: 9.0266\nEpoch [2/5], Step [0/3236], Loss: 1.9572, Perplexity: 7.0792\nEpoch [2/5], Step [10/3236], Loss: 1.9578, Perplexity: 7.0837\nEpoch [2/5], Step [20/3236], Loss: 1.9450, Perplexity: 6.9935\nEpoch [2/5], Step [30/3236], Loss: 2.0048, Perplexity: 7.4245\nEpoch [2/5], Step [40/3236], Loss: 1.9239, Perplexity: 6.8477\nEpoch [2/5], Step [50/3236], Loss: 2.0150, Perplexity: 7.5009\nEpoch [2/5], Step [60/3236], Loss: 1.9254, Perplexity: 6.8576\nEpoch [2/5], Step [70/3236], Loss: 2.0357, Perplexity: 7.6577\nEpoch [2/5], Step [80/3236], Loss: 1.9442, Perplexity: 6.9883\nEpoch [2/5], Step [90/3236], Loss: 1.9929, Perplexity: 7.3365\nEpoch [2/5], Step [100/3236], Loss: 1.8845, Perplexity: 6.5828\nEpoch [2/5], Step [110/3236], Loss: 1.9688, Perplexity: 7.1619\nEpoch [2/5], Step [120/3236], Loss: 1.9947, Perplexity: 7.3500\nEpoch [2/5], Step [130/3236], Loss: 2.0102, Perplexity: 7.4646\nEpoch [2/5], Step [140/3236], Loss: 1.8644, Perplexity: 6.4522\nEpoch [2/5], Step [150/3236], Loss: 1.9035, Perplexity: 6.7096\nEpoch [2/5], Step [160/3236], Loss: 2.0134, Perplexity: 7.4885\nEpoch [2/5], Step [170/3236], Loss: 1.9982, Perplexity: 7.3756\nEpoch [2/5], Step [180/3236], Loss: 1.9750, Perplexity: 7.2066\nEpoch [2/5], Step [190/3236], Loss: 1.9742, Perplexity: 7.2012\nEpoch [2/5], Step [200/3236], Loss: 2.0827, Perplexity: 8.0265\nEpoch [2/5], Step [210/3236], Loss: 1.8363, Perplexity: 6.2731\nEpoch [2/5], Step [220/3236], Loss: 1.9061, Perplexity: 6.7270\nEpoch [2/5], Step [230/3236], Loss: 1.9383, Perplexity: 6.9471\nEpoch [2/5], Step [240/3236], Loss: 1.9718, Perplexity: 7.1834\nEpoch [2/5], Step [250/3236], Loss: 1.8822, Perplexity: 6.5677\nEpoch [2/5], Step [260/3236], Loss: 1.9521, Perplexity: 7.0432\nEpoch [2/5], Step [270/3236], Loss: 2.0595, Perplexity: 7.8421\nEpoch [2/5], Step [280/3236], Loss: 1.9665, Perplexity: 7.1455\nEpoch [2/5], Step [290/3236], Loss: 2.1266, Perplexity: 8.3860\nEpoch [2/5], Step [300/3236], Loss: 2.1713, Perplexity: 8.7698\nEpoch [2/5], Step [310/3236], Loss: 1.9845, Perplexity: 7.2753\nEpoch [2/5], Step [320/3236], Loss: 2.0490, Perplexity: 7.7600\nEpoch [2/5], Step [330/3236], Loss: 2.1639, Perplexity: 8.7054\nEpoch [2/5], Step [340/3236], Loss: 1.9459, Perplexity: 6.9996\nEpoch [2/5], Step [350/3236], Loss: 2.0294, Perplexity: 7.6097\nEpoch [2/5], Step [360/3236], Loss: 2.0401, Perplexity: 7.6912\nEpoch [2/5], Step [370/3236], Loss: 1.9356, Perplexity: 6.9284\nEpoch [2/5], Step [380/3236], Loss: 1.9649, Perplexity: 7.1342\nEpoch [2/5], Step [390/3236], Loss: 2.1734, Perplexity: 8.7880\nEpoch [2/5], Step [400/3236], Loss: 1.9276, Perplexity: 6.8727\nEpoch [2/5], Step [410/3236], Loss: 1.9506, Perplexity: 7.0328\nEpoch [2/5], Step [420/3236], Loss: 2.0430, Perplexity: 7.7135\nEpoch [2/5], Step [430/3236], Loss: 1.9817, Perplexity: 7.2552\nEpoch [2/5], Step [440/3236], Loss: 1.9751, Perplexity: 7.2074\nEpoch [2/5], Step [450/3236], Loss: 1.9907, Perplexity: 7.3208\nEpoch [2/5], Step [460/3236], Loss: 2.0332, Perplexity: 7.6386\nEpoch [2/5], Step [470/3236], Loss: 2.0680, Perplexity: 7.9087\nEpoch [2/5], Step [480/3236], Loss: 2.0473, Perplexity: 7.7468\nEpoch [2/5], Step [490/3236], Loss: 2.0621, Perplexity: 7.8625\nEpoch [2/5], Step [500/3236], Loss: 1.8820, Perplexity: 6.5664\nEpoch [2/5], Step [510/3236], Loss: 1.9447, Perplexity: 6.9917\nEpoch [2/5], Step [520/3236], Loss: 2.0216, Perplexity: 7.5505\nEpoch [2/5], Step [530/3236], Loss: 1.9880, Perplexity: 7.3011\nEpoch [2/5], Step [540/3236], Loss: 1.9620, Perplexity: 7.1139\nEpoch [2/5], Step [550/3236], Loss: 1.8894, Perplexity: 6.6156\nEpoch [2/5], Step [560/3236], Loss: 2.0594, Perplexity: 7.8409\nEpoch [2/5], Step [570/3236], Loss: 1.9849, Perplexity: 7.2786\nEpoch [2/5], Step [580/3236], Loss: 2.0441, Perplexity: 7.7222\nEpoch [2/5], Step [590/3236], Loss: 1.9678, Perplexity: 7.1547\nEpoch [2/5], Step [600/3236], Loss: 2.0124, Perplexity: 7.4813\nEpoch [2/5], Step [610/3236], Loss: 1.8861, Perplexity: 6.5935\nEpoch [2/5], Step [620/3236], Loss: 2.0324, Perplexity: 7.6325\nEpoch [2/5], Step [630/3236], Loss: 2.0421, Perplexity: 7.7069\nEpoch [2/5], Step [640/3236], Loss: 1.9348, Perplexity: 6.9229\nEpoch [2/5], Step [650/3236], Loss: 2.0067, Perplexity: 7.4390\nEpoch [2/5], Step [660/3236], Loss: 1.9644, Perplexity: 7.1308\nEpoch [2/5], Step [670/3236], Loss: 1.8027, Perplexity: 6.0658\nEpoch [2/5], Step [680/3236], Loss: 2.0061, Perplexity: 7.4342\nEpoch [2/5], Step [690/3236], Loss: 2.0280, Perplexity: 7.5985\nEpoch [2/5], Step [700/3236], Loss: 1.9852, Perplexity: 7.2804\nEpoch [2/5], Step [710/3236], Loss: 1.9933, Perplexity: 7.3400\nEpoch [2/5], Step [720/3236], Loss: 1.9707, Perplexity: 7.1757\nEpoch [2/5], Step [730/3236], Loss: 1.9195, Perplexity: 6.8173\nEpoch [2/5], Step [740/3236], Loss: 2.0383, Perplexity: 7.6773\nEpoch [2/5], Step [750/3236], Loss: 1.9736, Perplexity: 7.1967\nEpoch [2/5], Step [760/3236], Loss: 1.9059, Perplexity: 6.7253\nEpoch [2/5], Step [770/3236], Loss: 2.1023, Perplexity: 8.1851\nEpoch [2/5], Step [780/3236], Loss: 1.8778, Perplexity: 6.5392\nEpoch [2/5], Step [790/3236], Loss: 2.0609, Perplexity: 7.8531\nEpoch [2/5], Step [800/3236], Loss: 2.1016, Perplexity: 8.1796\nEpoch [2/5], Step [810/3236], Loss: 1.8808, Perplexity: 6.5586\nEpoch [2/5], Step [820/3236], Loss: 2.0369, Perplexity: 7.6671\nEpoch [2/5], Step [830/3236], Loss: 1.9617, Perplexity: 7.1115\nEpoch [2/5], Step [840/3236], Loss: 2.1066, Perplexity: 8.2206\nEpoch [2/5], Step [850/3236], Loss: 2.0542, Perplexity: 7.8004\nEpoch [2/5], Step [860/3236], Loss: 2.0037, Perplexity: 7.4164\nEpoch [2/5], Step [870/3236], Loss: 1.9186, Perplexity: 6.8111\nEpoch [2/5], Step [880/3236], Loss: 2.0007, Perplexity: 7.3943\nEpoch [2/5], Step [890/3236], Loss: 2.1193, Perplexity: 8.3255\nEpoch [2/5], Step [900/3236], Loss: 2.0228, Perplexity: 7.5598\nEpoch [2/5], Step [910/3236], Loss: 1.9375, Perplexity: 6.9413\nEpoch [2/5], Step [920/3236], Loss: 1.9722, Perplexity: 7.1863\nEpoch [2/5], Step [930/3236], Loss: 2.0118, Perplexity: 7.4770\nEpoch [2/5], Step [940/3236], Loss: 1.9506, Perplexity: 7.0327\nEpoch [2/5], Step [950/3236], Loss: 1.9595, Perplexity: 7.0955\nEpoch [2/5], Step [960/3236], Loss: 1.9929, Perplexity: 7.3366\nEpoch [2/5], Step [970/3236], Loss: 1.8442, Perplexity: 6.3228\nEpoch [2/5], Step [980/3236], Loss: 1.9566, Perplexity: 7.0751\nEpoch [2/5], Step [990/3236], Loss: 1.8894, Perplexity: 6.6152\nEpoch [2/5], Step [1000/3236], Loss: 1.9627, Perplexity: 7.1186\nEpoch [2/5], Step [1010/3236], Loss: 1.9828, Perplexity: 7.2630\nEpoch [2/5], Step [1020/3236], Loss: 1.9458, Perplexity: 6.9991\nEpoch [2/5], Step [1030/3236], Loss: 2.0792, Perplexity: 7.9979\nEpoch [2/5], Step [1040/3236], Loss: 1.8779, Perplexity: 6.5397\nEpoch [2/5], Step [1050/3236], Loss: 2.0940, Perplexity: 8.1175\nEpoch [2/5], Step [1060/3236], Loss: 2.0389, Perplexity: 7.6819\nEpoch [2/5], Step [1070/3236], Loss: 2.1000, Perplexity: 8.1662\nEpoch [2/5], Step [1080/3236], Loss: 2.1536, Perplexity: 8.6160\nEpoch [2/5], Step [1090/3236], Loss: 2.0494, Perplexity: 7.7630\nEpoch [2/5], Step [1100/3236], Loss: 1.9022, Perplexity: 6.7008\nEpoch [2/5], Step [1110/3236], Loss: 1.9821, Perplexity: 7.2578\nEpoch [2/5], Step [1120/3236], Loss: 2.0711, Perplexity: 7.9337\nEpoch [2/5], Step [1130/3236], Loss: 2.0320, Perplexity: 7.6293\nEpoch [2/5], Step [1140/3236], Loss: 1.9986, Perplexity: 7.3790\nEpoch [2/5], Step [1150/3236], Loss: 1.9707, Perplexity: 7.1754\nEpoch [2/5], Step [1160/3236], Loss: 2.0537, Perplexity: 7.7969\nEpoch [2/5], Step [1170/3236], Loss: 2.0382, Perplexity: 7.6767\nEpoch [2/5], Step [1180/3236], Loss: 1.9914, Perplexity: 7.3261\nEpoch [2/5], Step [1190/3236], Loss: 2.0298, Perplexity: 7.6125\nEpoch [2/5], Step [1200/3236], Loss: 2.0408, Perplexity: 7.6970\nEpoch [2/5], Step [1210/3236], Loss: 2.1088, Perplexity: 8.2380\nEpoch [2/5], Step [1220/3236], Loss: 2.0744, Perplexity: 7.9600\nEpoch [2/5], Step [1230/3236], Loss: 2.0139, Perplexity: 7.4922\nEpoch [2/5], Step [1240/3236], Loss: 2.0666, Perplexity: 7.8975\nEpoch [2/5], Step [1250/3236], Loss: 2.0806, Perplexity: 8.0095\nEpoch [2/5], Step [1260/3236], Loss: 1.9414, Perplexity: 6.9687\nEpoch [2/5], Step [1270/3236], Loss: 2.0124, Perplexity: 7.4809\nEpoch [2/5], Step [1280/3236], Loss: 2.0042, Perplexity: 7.4204\nEpoch [2/5], Step [1290/3236], Loss: 1.9189, Perplexity: 6.8135\nEpoch [2/5], Step [1300/3236], Loss: 1.8986, Perplexity: 6.6763\nEpoch [2/5], Step [1310/3236], Loss: 2.0429, Perplexity: 7.7130\nEpoch [2/5], Step [1320/3236], Loss: 1.9349, Perplexity: 6.9235\nEpoch [2/5], Step [1330/3236], Loss: 1.9858, Perplexity: 7.2847\nEpoch [2/5], Step [1340/3236], Loss: 2.0118, Perplexity: 7.4766\nEpoch [2/5], Step [1350/3236], Loss: 1.9537, Perplexity: 7.0550\nEpoch [2/5], Step [1360/3236], Loss: 2.0100, Perplexity: 7.4632\nEpoch [2/5], Step [1370/3236], Loss: 2.0063, Perplexity: 7.4360\nEpoch [2/5], Step [1380/3236], Loss: 2.1435, Perplexity: 8.5296\nEpoch [2/5], Step [1390/3236], Loss: 2.0363, Perplexity: 7.6625\nEpoch [2/5], Step [1400/3236], Loss: 2.0157, Perplexity: 7.5060\nEpoch [2/5], Step [1410/3236], Loss: 1.8605, Perplexity: 6.4271\nEpoch [2/5], Step [1420/3236], Loss: 2.0817, Perplexity: 8.0180\nEpoch [2/5], Step [1430/3236], Loss: 2.0304, Perplexity: 7.6175\nEpoch [2/5], Step [1440/3236], Loss: 2.0495, Perplexity: 7.7638\nEpoch [2/5], Step [1450/3236], Loss: 1.8869, Perplexity: 6.5987\nEpoch [2/5], Step [1460/3236], Loss: 1.9645, Perplexity: 7.1312\nEpoch [2/5], Step [1470/3236], Loss: 2.0131, Perplexity: 7.4865\nEpoch [2/5], Step [1480/3236], Loss: 2.0256, Perplexity: 7.5803\nEpoch [2/5], Step [1490/3236], Loss: 1.9524, Perplexity: 7.0455\nEpoch [2/5], Step [1500/3236], Loss: 2.1310, Perplexity: 8.4237\nEpoch [2/5], Step [1510/3236], Loss: 1.9482, Perplexity: 7.0163\nEpoch [2/5], Step [1520/3236], Loss: 2.0578, Perplexity: 7.8290\nEpoch [2/5], Step [1530/3236], Loss: 2.0277, Perplexity: 7.5968\nEpoch [2/5], Step [1540/3236], Loss: 1.9127, Perplexity: 6.7714\nEpoch [2/5], Step [1550/3236], Loss: 1.9657, Perplexity: 7.1402\nEpoch [2/5], Step [1560/3236], Loss: 1.9413, Perplexity: 6.9681\nEpoch [2/5], Step [1570/3236], Loss: 2.0302, Perplexity: 7.6153\nEpoch [2/5], Step [1580/3236], Loss: 1.8727, Perplexity: 6.5057\nEpoch [2/5], Step [1590/3236], Loss: 2.0056, Perplexity: 7.4304\nEpoch [2/5], Step [1600/3236], Loss: 1.9138, Perplexity: 6.7789\nEpoch [2/5], Step [1610/3236], Loss: 2.0416, Perplexity: 7.7033\nEpoch [2/5], Step [1620/3236], Loss: 1.9534, Perplexity: 7.0527\nEpoch [2/5], Step [1630/3236], Loss: 2.0788, Perplexity: 7.9948\nEpoch [2/5], Step [1640/3236], Loss: 1.9501, Perplexity: 7.0297\nEpoch [2/5], Step [1650/3236], Loss: 1.9823, Perplexity: 7.2592\nEpoch [2/5], Step [1660/3236], Loss: 2.1098, Perplexity: 8.2462\nEpoch [2/5], Step [1670/3236], Loss: 2.0768, Perplexity: 7.9788\nEpoch [2/5], Step [1680/3236], Loss: 2.0691, Perplexity: 7.9178\nEpoch [2/5], Step [1690/3236], Loss: 2.1232, Perplexity: 8.3582\nEpoch [2/5], Step [1700/3236], Loss: 2.0124, Perplexity: 7.4814\nEpoch [2/5], Step [1710/3236], Loss: 2.1284, Perplexity: 8.4015\nEpoch [2/5], Step [1720/3236], Loss: 1.9366, Perplexity: 6.9351\nEpoch [2/5], Step [1730/3236], Loss: 1.9637, Perplexity: 7.1256\nEpoch [2/5], Step [1740/3236], Loss: 1.9758, Perplexity: 7.2122\nEpoch [2/5], Step [1750/3236], Loss: 1.9731, Perplexity: 7.1931\nEpoch [2/5], Step [1760/3236], Loss: 1.9289, Perplexity: 6.8822\nEpoch [2/5], Step [1770/3236], Loss: 2.0266, Perplexity: 7.5881\nEpoch [2/5], Step [1780/3236], Loss: 1.9528, Perplexity: 7.0482\nEpoch [2/5], Step [1790/3236], Loss: 1.9992, Perplexity: 7.3829\nEpoch [2/5], Step [1800/3236], Loss: 1.9326, Perplexity: 6.9075\nEpoch [2/5], Step [1810/3236], Loss: 1.9971, Perplexity: 7.3674\nEpoch [2/5], Step [1820/3236], Loss: 1.9518, Perplexity: 7.0416\nEpoch [2/5], Step [1830/3236], Loss: 2.0348, Perplexity: 7.6504\nEpoch [2/5], Step [1840/3236], Loss: 1.9727, Perplexity: 7.1899\nEpoch [2/5], Step [1850/3236], Loss: 1.9998, Perplexity: 7.3876\nEpoch [2/5], Step [1860/3236], Loss: 1.9119, Perplexity: 6.7658\nEpoch [2/5], Step [1870/3236], Loss: 1.9622, Perplexity: 7.1146\nEpoch [2/5], Step [1880/3236], Loss: 1.9060, Perplexity: 6.7261\nEpoch [2/5], Step [1890/3236], Loss: 1.9536, Perplexity: 7.0543\nEpoch [2/5], Step [1900/3236], Loss: 2.0087, Perplexity: 7.4535\nEpoch [2/5], Step [1910/3236], Loss: 1.9797, Perplexity: 7.2407\nEpoch [2/5], Step [1920/3236], Loss: 2.1450, Perplexity: 8.5422\nEpoch [2/5], Step [1930/3236], Loss: 2.0316, Perplexity: 7.6261\nEpoch [2/5], Step [1940/3236], Loss: 2.0391, Perplexity: 7.6840\nEpoch [2/5], Step [1950/3236], Loss: 1.9533, Perplexity: 7.0520\nEpoch [2/5], Step [1960/3236], Loss: 1.9572, Perplexity: 7.0794\nEpoch [2/5], Step [1970/3236], Loss: 2.0827, Perplexity: 8.0262\nEpoch [2/5], Step [1980/3236], Loss: 2.1079, Perplexity: 8.2313\nEpoch [2/5], Step [1990/3236], Loss: 2.0340, Perplexity: 7.6442\nEpoch [2/5], Step [2000/3236], Loss: 2.0217, Perplexity: 7.5515\nEpoch [2/5], Step [2010/3236], Loss: 1.9839, Perplexity: 7.2713\nEpoch [2/5], Step [2020/3236], Loss: 1.9750, Perplexity: 7.2064\nEpoch [2/5], Step [2030/3236], Loss: 2.1418, Perplexity: 8.5143\nEpoch [2/5], Step [2040/3236], Loss: 1.8565, Perplexity: 6.4013\nEpoch [2/5], Step [2050/3236], Loss: 1.9955, Perplexity: 7.3559\nEpoch [2/5], Step [2060/3236], Loss: 2.0365, Perplexity: 7.6638\nEpoch [2/5], Step [2070/3236], Loss: 2.0212, Perplexity: 7.5474\nEpoch [2/5], Step [2080/3236], Loss: 2.0387, Perplexity: 7.6806\nEpoch [2/5], Step [2090/3236], Loss: 1.9703, Perplexity: 7.1728\nEpoch [2/5], Step [2100/3236], Loss: 1.8734, Perplexity: 6.5102\nEpoch [2/5], Step [2110/3236], Loss: 2.0725, Perplexity: 7.9450\nEpoch [2/5], Step [2120/3236], Loss: 1.9468, Perplexity: 7.0059\nEpoch [2/5], Step [2130/3236], Loss: 2.0459, Perplexity: 7.7360\nEpoch [2/5], Step [2140/3236], Loss: 2.0088, Perplexity: 7.4546\nEpoch [2/5], Step [2150/3236], Loss: 1.9504, Perplexity: 7.0317\nEpoch [2/5], Step [2160/3236], Loss: 1.9402, Perplexity: 6.9599\nEpoch [2/5], Step [2170/3236], Loss: 2.0848, Perplexity: 8.0431\nEpoch [2/5], Step [2180/3236], Loss: 2.0160, Perplexity: 7.5082\nEpoch [2/5], Step [2190/3236], Loss: 2.0271, Perplexity: 7.5924\nEpoch [2/5], Step [2200/3236], Loss: 2.1699, Perplexity: 8.7575\nEpoch [2/5], Step [2210/3236], Loss: 2.1174, Perplexity: 8.3091\nEpoch [2/5], Step [2220/3236], Loss: 1.9904, Perplexity: 7.3182\nEpoch [2/5], Step [2230/3236], Loss: 1.9774, Perplexity: 7.2236\nEpoch [2/5], Step [2240/3236], Loss: 1.9774, Perplexity: 7.2241\nEpoch [2/5], Step [2250/3236], Loss: 2.0531, Perplexity: 7.7923\nEpoch [2/5], Step [2260/3236], Loss: 2.0140, Perplexity: 7.4935\nEpoch [2/5], Step [2270/3236], Loss: 1.9180, Perplexity: 6.8077\nEpoch [2/5], Step [2280/3236], Loss: 1.8883, Perplexity: 6.6080\nEpoch [2/5], Step [2290/3236], Loss: 1.9815, Perplexity: 7.2533\nEpoch [2/5], Step [2300/3236], Loss: 1.9800, Perplexity: 7.2429\nEpoch [2/5], Step [2310/3236], Loss: 1.9943, Perplexity: 7.3472\nEpoch [2/5], Step [2320/3236], Loss: 1.9683, Perplexity: 7.1584\nEpoch [2/5], Step [2330/3236], Loss: 2.0105, Perplexity: 7.4672\nEpoch [2/5], Step [2340/3236], Loss: 2.1349, Perplexity: 8.4561\nEpoch [2/5], Step [2350/3236], Loss: 1.9260, Perplexity: 6.8621\nEpoch [2/5], Step [2360/3236], Loss: 2.0141, Perplexity: 7.4941\nEpoch [2/5], Step [2370/3236], Loss: 2.0714, Perplexity: 7.9356\nEpoch [2/5], Step [2380/3236], Loss: 1.8415, Perplexity: 6.3058\nEpoch [2/5], Step [2390/3236], Loss: 2.0105, Perplexity: 7.4669\nEpoch [2/5], Step [2400/3236], Loss: 2.0624, Perplexity: 7.8645\nEpoch [2/5], Step [2410/3236], Loss: 1.9560, Perplexity: 7.0713\nEpoch [2/5], Step [2420/3236], Loss: 1.9178, Perplexity: 6.8058\nEpoch [2/5], Step [2430/3236], Loss: 1.9306, Perplexity: 6.8935\nEpoch [2/5], Step [2440/3236], Loss: 2.0343, Perplexity: 7.6469\nEpoch [2/5], Step [2450/3236], Loss: 1.9580, Perplexity: 7.0849\nEpoch [2/5], Step [2460/3236], Loss: 2.0779, Perplexity: 7.9877\nEpoch [2/5], Step [2470/3236], Loss: 2.0921, Perplexity: 8.1022\nEpoch [2/5], Step [2480/3236], Loss: 2.0381, Perplexity: 7.6758\nEpoch [2/5], Step [2490/3236], Loss: 2.0015, Perplexity: 7.4005\nEpoch [2/5], Step [2500/3236], Loss: 1.9574, Perplexity: 7.0808\nEpoch [2/5], Step [2510/3236], Loss: 1.9695, Perplexity: 7.1672\nEpoch [2/5], Step [2520/3236], Loss: 1.8665, Perplexity: 6.4653\nEpoch [2/5], Step [2530/3236], Loss: 2.0443, Perplexity: 7.7241\nEpoch [2/5], Step [2540/3236], Loss: 2.0997, Perplexity: 8.1634\nEpoch [2/5], Step [2550/3236], Loss: 2.0319, Perplexity: 7.6289\nEpoch [2/5], Step [2560/3236], Loss: 1.9690, Perplexity: 7.1638\nEpoch [2/5], Step [2570/3236], Loss: 2.1269, Perplexity: 8.3888\nEpoch [2/5], Step [2580/3236], Loss: 1.9753, Perplexity: 7.2088\nEpoch [2/5], Step [2590/3236], Loss: 1.9313, Perplexity: 6.8988\nEpoch [2/5], Step [2600/3236], Loss: 1.8678, Perplexity: 6.4741\nEpoch [2/5], Step [2610/3236], Loss: 2.0417, Perplexity: 7.7038\nEpoch [2/5], Step [2620/3236], Loss: 2.1924, Perplexity: 8.9569\nEpoch [2/5], Step [2630/3236], Loss: 1.9803, Perplexity: 7.2449\nEpoch [2/5], Step [2640/3236], Loss: 1.9439, Perplexity: 6.9859\nEpoch [2/5], Step [2650/3236], Loss: 2.0920, Perplexity: 8.1010\nEpoch [2/5], Step [2660/3236], Loss: 1.8843, Perplexity: 6.5816\nEpoch [2/5], Step [2670/3236], Loss: 1.9109, Perplexity: 6.7589\nEpoch [2/5], Step [2680/3236], Loss: 2.0310, Perplexity: 7.6217\nEpoch [2/5], Step [2690/3236], Loss: 1.8986, Perplexity: 6.6765\nEpoch [2/5], Step [2700/3236], Loss: 1.8296, Perplexity: 6.2312\nEpoch [2/5], Step [2710/3236], Loss: 2.0958, Perplexity: 8.1320\nEpoch [2/5], Step [2720/3236], Loss: 1.9455, Perplexity: 6.9974\nEpoch [2/5], Step [2730/3236], Loss: 2.0724, Perplexity: 7.9435\nEpoch [2/5], Step [2740/3236], Loss: 2.0327, Perplexity: 7.6350\nEpoch [2/5], Step [2750/3236], Loss: 2.0363, Perplexity: 7.6624\nEpoch [2/5], Step [2760/3236], Loss: 1.9931, Perplexity: 7.3384\nEpoch [2/5], Step [2770/3236], Loss: 2.0092, Perplexity: 7.4573\nEpoch [2/5], Step [2780/3236], Loss: 2.0595, Perplexity: 7.8417\nEpoch [2/5], Step [2790/3236], Loss: 1.9325, Perplexity: 6.9071\nEpoch [2/5], Step [2800/3236], Loss: 2.0919, Perplexity: 8.1006\nEpoch [2/5], Step [2810/3236], Loss: 2.0550, Perplexity: 7.8070\nEpoch [2/5], Step [2820/3236], Loss: 2.0531, Perplexity: 7.7921\nEpoch [2/5], Step [2830/3236], Loss: 1.9956, Perplexity: 7.3570\nEpoch [2/5], Step [2840/3236], Loss: 2.1229, Perplexity: 8.3550\nEpoch [2/5], Step [2850/3236], Loss: 1.9092, Perplexity: 6.7475\nEpoch [2/5], Step [2860/3236], Loss: 1.9761, Perplexity: 7.2148\nEpoch [2/5], Step [2870/3236], Loss: 2.1350, Perplexity: 8.4574\nEpoch [2/5], Step [2880/3236], Loss: 1.9280, Perplexity: 6.8759\nEpoch [2/5], Step [2890/3236], Loss: 2.0599, Perplexity: 7.8450\nEpoch [2/5], Step [2900/3236], Loss: 2.0095, Perplexity: 7.4599\nEpoch [2/5], Step [2910/3236], Loss: 2.0108, Perplexity: 7.4690\nEpoch [2/5], Step [2920/3236], Loss: 2.0278, Perplexity: 7.5976\nEpoch [2/5], Step [2930/3236], Loss: 2.0041, Perplexity: 7.4192\nEpoch [2/5], Step [2940/3236], Loss: 2.0528, Perplexity: 7.7894\nEpoch [2/5], Step [2950/3236], Loss: 1.9705, Perplexity: 7.1741\nEpoch [2/5], Step [2960/3236], Loss: 2.1488, Perplexity: 8.5747\nEpoch [2/5], Step [2970/3236], Loss: 2.0826, Perplexity: 8.0255\nEpoch [2/5], Step [2980/3236], Loss: 2.0187, Perplexity: 7.5286\nEpoch [2/5], Step [2990/3236], Loss: 2.0266, Perplexity: 7.5886\nEpoch [2/5], Step [3000/3236], Loss: 2.0035, Perplexity: 7.4147\nEpoch [2/5], Step [3010/3236], Loss: 2.0349, Perplexity: 7.6512\nEpoch [2/5], Step [3020/3236], Loss: 1.9866, Perplexity: 7.2909\nEpoch [2/5], Step [3030/3236], Loss: 1.8869, Perplexity: 6.5991\nEpoch [2/5], Step [3040/3236], Loss: 1.9793, Perplexity: 7.2378\nEpoch [2/5], Step [3050/3236], Loss: 2.0219, Perplexity: 7.5524\nEpoch [2/5], Step [3060/3236], Loss: 1.9497, Perplexity: 7.0265\nEpoch [2/5], Step [3070/3236], Loss: 1.8773, Perplexity: 6.5357\nEpoch [2/5], Step [3080/3236], Loss: 1.9743, Perplexity: 7.2018\nEpoch [2/5], Step [3090/3236], Loss: 2.0512, Perplexity: 7.7776\nEpoch [2/5], Step [3100/3236], Loss: 2.0251, Perplexity: 7.5768\nEpoch [2/5], Step [3110/3236], Loss: 2.1318, Perplexity: 8.4302\nEpoch [2/5], Step [3120/3236], Loss: 1.9842, Perplexity: 7.2733\nEpoch [2/5], Step [3130/3236], Loss: 1.9741, Perplexity: 7.2005\nEpoch [2/5], Step [3140/3236], Loss: 1.8773, Perplexity: 6.5357\nEpoch [2/5], Step [3150/3236], Loss: 1.9722, Perplexity: 7.1868\nEpoch [2/5], Step [3160/3236], Loss: 2.0477, Perplexity: 7.7498\nEpoch [2/5], Step [3170/3236], Loss: 2.0119, Perplexity: 7.4775\nEpoch [2/5], Step [3180/3236], Loss: 1.9617, Perplexity: 7.1115\nEpoch [2/5], Step [3190/3236], Loss: 2.0362, Perplexity: 7.6618\nEpoch [2/5], Step [3200/3236], Loss: 2.0084, Perplexity: 7.4515\nEpoch [2/5], Step [3210/3236], Loss: 2.1256, Perplexity: 8.3783\nEpoch [2/5], Step [3220/3236], Loss: 2.0362, Perplexity: 7.6615\nEpoch [2/5], Step [3230/3236], Loss: 1.9024, Perplexity: 6.7019\nEpoch [3/5], Step [0/3236], Loss: 1.7936, Perplexity: 6.0111\nEpoch [3/5], Step [10/3236], Loss: 1.9265, Perplexity: 6.8656\nEpoch [3/5], Step [20/3236], Loss: 1.9616, Perplexity: 7.1107\nEpoch [3/5], Step [30/3236], Loss: 1.8763, Perplexity: 6.5293\nEpoch [3/5], Step [40/3236], Loss: 1.9223, Perplexity: 6.8369\nEpoch [3/5], Step [50/3236], Loss: 1.9134, Perplexity: 6.7762\nEpoch [3/5], Step [60/3236], Loss: 1.8642, Perplexity: 6.4506\nEpoch [3/5], Step [70/3236], Loss: 1.8111, Perplexity: 6.1174\nEpoch [3/5], Step [80/3236], Loss: 1.9647, Perplexity: 7.1325\nEpoch [3/5], Step [90/3236], Loss: 1.8438, Perplexity: 6.3208\nEpoch [3/5], Step [100/3236], Loss: 1.9458, Perplexity: 6.9992\nEpoch [3/5], Step [110/3236], Loss: 1.7448, Perplexity: 5.7245\nEpoch [3/5], Step [120/3236], Loss: 1.8366, Perplexity: 6.2752\nEpoch [3/5], Step [130/3236], Loss: 1.8838, Perplexity: 6.5782\nEpoch [3/5], Step [140/3236], Loss: 1.9855, Perplexity: 7.2825\nEpoch [3/5], Step [150/3236], Loss: 1.8607, Perplexity: 6.4281\nEpoch [3/5], Step [160/3236], Loss: 1.8796, Perplexity: 6.5508\nEpoch [3/5], Step [170/3236], Loss: 1.7853, Perplexity: 5.9616\nEpoch [3/5], Step [180/3236], Loss: 1.9255, Perplexity: 6.8588\nEpoch [3/5], Step [190/3236], Loss: 1.9664, Perplexity: 7.1450\nEpoch [3/5], Step [200/3236], Loss: 1.9271, Perplexity: 6.8698\nEpoch [3/5], Step [210/3236], Loss: 1.9760, Perplexity: 7.2137\nEpoch [3/5], Step [220/3236], Loss: 1.9390, Perplexity: 6.9515\nEpoch [3/5], Step [230/3236], Loss: 1.8915, Perplexity: 6.6290\nEpoch [3/5], Step [240/3236], Loss: 1.8849, Perplexity: 6.5856\nEpoch [3/5], Step [250/3236], Loss: 1.8330, Perplexity: 6.2524\nEpoch [3/5], Step [260/3236], Loss: 1.9649, Perplexity: 7.1344\nEpoch [3/5], Step [270/3236], Loss: 1.8999, Perplexity: 6.6855\nEpoch [3/5], Step [280/3236], Loss: 1.8267, Perplexity: 6.2131\nEpoch [3/5], Step [290/3236], Loss: 1.8574, Perplexity: 6.4073\nEpoch [3/5], Step [300/3236], Loss: 1.8778, Perplexity: 6.5388\nEpoch [3/5], Step [310/3236], Loss: 1.9123, Perplexity: 6.7689\nEpoch [3/5], Step [320/3236], Loss: 1.9167, Perplexity: 6.7986\nEpoch [3/5], Step [330/3236], Loss: 1.7958, Perplexity: 6.0244\nEpoch [3/5], Step [340/3236], Loss: 2.0134, Perplexity: 7.4888\nEpoch [3/5], Step [350/3236], Loss: 1.9549, Perplexity: 7.0633\nEpoch [3/5], Step [360/3236], Loss: 1.9745, Perplexity: 7.2032\nEpoch [3/5], Step [370/3236], Loss: 1.8774, Perplexity: 6.5364\nEpoch [3/5], Step [380/3236], Loss: 2.0339, Perplexity: 7.6435\nEpoch [3/5], Step [390/3236], Loss: 1.9292, Perplexity: 6.8843\nEpoch [3/5], Step [400/3236], Loss: 1.9473, Perplexity: 7.0100\nEpoch [3/5], Step [410/3236], Loss: 1.8465, Perplexity: 6.3379\nEpoch [3/5], Step [420/3236], Loss: 1.9491, Perplexity: 7.0226\nEpoch [3/5], Step [430/3236], Loss: 1.8692, Perplexity: 6.4834\nEpoch [3/5], Step [440/3236], Loss: 1.9077, Perplexity: 6.7374\nEpoch [3/5], Step [450/3236], Loss: 1.9227, Perplexity: 6.8392\nEpoch [3/5], Step [460/3236], Loss: 1.9276, Perplexity: 6.8730\nEpoch [3/5], Step [470/3236], Loss: 1.9048, Perplexity: 6.7179\nEpoch [3/5], Step [480/3236], Loss: 1.9547, Perplexity: 7.0617\nEpoch [3/5], Step [490/3236], Loss: 1.8329, Perplexity: 6.2522\nEpoch [3/5], Step [500/3236], Loss: 1.9716, Perplexity: 7.1822\nEpoch [3/5], Step [510/3236], Loss: 1.8755, Perplexity: 6.5244\nEpoch [3/5], Step [520/3236], Loss: 1.8794, Perplexity: 6.5494\nEpoch [3/5], Step [530/3236], Loss: 1.9484, Perplexity: 7.0172\nEpoch [3/5], Step [540/3236], Loss: 1.9295, Perplexity: 6.8863\nEpoch [3/5], Step [550/3236], Loss: 1.9746, Perplexity: 7.2035\nEpoch [3/5], Step [560/3236], Loss: 1.8265, Perplexity: 6.2121\nEpoch [3/5], Step [570/3236], Loss: 1.9191, Perplexity: 6.8150\nEpoch [3/5], Step [580/3236], Loss: 1.8047, Perplexity: 6.0782\nEpoch [3/5], Step [590/3236], Loss: 1.7903, Perplexity: 5.9915\nEpoch [3/5], Step [600/3236], Loss: 1.8649, Perplexity: 6.4553\nEpoch [3/5], Step [610/3236], Loss: 1.8926, Perplexity: 6.6367\nEpoch [3/5], Step [620/3236], Loss: 1.9606, Perplexity: 7.1033\nEpoch [3/5], Step [630/3236], Loss: 2.0257, Perplexity: 7.5817\nEpoch [3/5], Step [640/3236], Loss: 1.8764, Perplexity: 6.5299\nEpoch [3/5], Step [650/3236], Loss: 1.9368, Perplexity: 6.9366\nEpoch [3/5], Step [660/3236], Loss: 1.9668, Perplexity: 7.1475\nEpoch [3/5], Step [670/3236], Loss: 1.9437, Perplexity: 6.9849\nEpoch [3/5], Step [680/3236], Loss: 1.8994, Perplexity: 6.6817\nEpoch [3/5], Step [690/3236], Loss: 1.8847, Perplexity: 6.5846\nEpoch [3/5], Step [700/3236], Loss: 1.9302, Perplexity: 6.8908\nEpoch [3/5], Step [710/3236], Loss: 1.9059, Perplexity: 6.7257\nEpoch [3/5], Step [720/3236], Loss: 2.0030, Perplexity: 7.4112\nEpoch [3/5], Step [730/3236], Loss: 1.9281, Perplexity: 6.8766\nEpoch [3/5], Step [740/3236], Loss: 1.9217, Perplexity: 6.8329\nEpoch [3/5], Step [750/3236], Loss: 2.0100, Perplexity: 7.4636\nEpoch [3/5], Step [760/3236], Loss: 1.9016, Perplexity: 6.6963\nEpoch [3/5], Step [770/3236], Loss: 1.9114, Perplexity: 6.7625\nEpoch [3/5], Step [780/3236], Loss: 1.8708, Perplexity: 6.4933\nEpoch [3/5], Step [790/3236], Loss: 1.9273, Perplexity: 6.8712\nEpoch [3/5], Step [800/3236], Loss: 1.8156, Perplexity: 6.1448\nEpoch [3/5], Step [810/3236], Loss: 1.8910, Perplexity: 6.6261\nEpoch [3/5], Step [820/3236], Loss: 1.8266, Perplexity: 6.2127\nEpoch [3/5], Step [830/3236], Loss: 1.9022, Perplexity: 6.7008\nEpoch [3/5], Step [840/3236], Loss: 1.9670, Perplexity: 7.1495\nEpoch [3/5], Step [850/3236], Loss: 1.9831, Perplexity: 7.2655\nEpoch [3/5], Step [860/3236], Loss: 1.8583, Perplexity: 6.4127\nEpoch [3/5], Step [870/3236], Loss: 1.9091, Perplexity: 6.7473\nEpoch [3/5], Step [880/3236], Loss: 1.9742, Perplexity: 7.2010\nEpoch [3/5], Step [890/3236], Loss: 2.0265, Perplexity: 7.5876\nEpoch [3/5], Step [900/3236], Loss: 1.8429, Perplexity: 6.3150\nEpoch [3/5], Step [910/3236], Loss: 1.8947, Perplexity: 6.6509\nEpoch [3/5], Step [920/3236], Loss: 1.9216, Perplexity: 6.8318\nEpoch [3/5], Step [930/3236], Loss: 1.9650, Perplexity: 7.1349\nEpoch [3/5], Step [940/3236], Loss: 1.7246, Perplexity: 5.6100\nEpoch [3/5], Step [950/3236], Loss: 1.9589, Perplexity: 7.0914\nEpoch [3/5], Step [960/3236], Loss: 1.8494, Perplexity: 6.3561\nEpoch [3/5], Step [970/3236], Loss: 2.0633, Perplexity: 7.8721\nEpoch [3/5], Step [980/3236], Loss: 1.8973, Perplexity: 6.6676\nEpoch [3/5], Step [990/3236], Loss: 1.8663, Perplexity: 6.4643\nEpoch [3/5], Step [1000/3236], Loss: 1.8079, Perplexity: 6.0976\nEpoch [3/5], Step [1010/3236], Loss: 1.9056, Perplexity: 6.7237\nEpoch [3/5], Step [1020/3236], Loss: 1.9169, Perplexity: 6.7996\nEpoch [3/5], Step [1030/3236], Loss: 1.8526, Perplexity: 6.3764\nEpoch [3/5], Step [1040/3236], Loss: 1.8729, Perplexity: 6.5074\nEpoch [3/5], Step [1050/3236], Loss: 1.8546, Perplexity: 6.3891\nEpoch [3/5], Step [1060/3236], Loss: 1.9385, Perplexity: 6.9484\nEpoch [3/5], Step [1070/3236], Loss: 1.8599, Perplexity: 6.4233\nEpoch [3/5], Step [1080/3236], Loss: 1.9281, Perplexity: 6.8763\nEpoch [3/5], Step [1090/3236], Loss: 1.8595, Perplexity: 6.4208\nEpoch [3/5], Step [1100/3236], Loss: 1.9618, Perplexity: 7.1118\nEpoch [3/5], Step [1110/3236], Loss: 1.9897, Perplexity: 7.3135\nEpoch [3/5], Step [1120/3236], Loss: 1.9268, Perplexity: 6.8676\nEpoch [3/5], Step [1130/3236], Loss: 1.9784, Perplexity: 7.2315\nEpoch [3/5], Step [1140/3236], Loss: 2.0397, Perplexity: 7.6882\nEpoch [3/5], Step [1150/3236], Loss: 1.8789, Perplexity: 6.5460\nEpoch [3/5], Step [1160/3236], Loss: 1.9361, Perplexity: 6.9314\nEpoch [3/5], Step [1170/3236], Loss: 1.8970, Perplexity: 6.6658\nEpoch [3/5], Step [1180/3236], Loss: 1.8918, Perplexity: 6.6313\nEpoch [3/5], Step [1190/3236], Loss: 1.8024, Perplexity: 6.0641\nEpoch [3/5], Step [1200/3236], Loss: 1.7303, Perplexity: 5.6423\nEpoch [3/5], Step [1210/3236], Loss: 1.9636, Perplexity: 7.1249\nEpoch [3/5], Step [1220/3236], Loss: 1.7875, Perplexity: 5.9745\nEpoch [3/5], Step [1230/3236], Loss: 1.9021, Perplexity: 6.7001\nEpoch [3/5], Step [1240/3236], Loss: 1.9618, Perplexity: 7.1118\nEpoch [3/5], Step [1250/3236], Loss: 2.0307, Perplexity: 7.6194\nEpoch [3/5], Step [1260/3236], Loss: 1.9471, Perplexity: 7.0086\nEpoch [3/5], Step [1270/3236], Loss: 1.8392, Perplexity: 6.2916\nEpoch [3/5], Step [1280/3236], Loss: 1.8481, Perplexity: 6.3475\nEpoch [3/5], Step [1290/3236], Loss: 1.9377, Perplexity: 6.9429\nEpoch [3/5], Step [1300/3236], Loss: 1.8810, Perplexity: 6.5598\nEpoch [3/5], Step [1310/3236], Loss: 1.9779, Perplexity: 7.2276\nEpoch [3/5], Step [1320/3236], Loss: 1.8762, Perplexity: 6.5283\nEpoch [3/5], Step [1330/3236], Loss: 1.8437, Perplexity: 6.3197\nEpoch [3/5], Step [1340/3236], Loss: 1.8965, Perplexity: 6.6624\nEpoch [3/5], Step [1350/3236], Loss: 1.9598, Perplexity: 7.0977\nEpoch [3/5], Step [1360/3236], Loss: 1.8877, Perplexity: 6.6040\nEpoch [3/5], Step [1370/3236], Loss: 1.9199, Perplexity: 6.8199\nEpoch [3/5], Step [1380/3236], Loss: 1.9219, Perplexity: 6.8340\nEpoch [3/5], Step [1390/3236], Loss: 1.9519, Perplexity: 7.0417\nEpoch [3/5], Step [1400/3236], Loss: 1.9246, Perplexity: 6.8525\nEpoch [3/5], Step [1410/3236], Loss: 1.9668, Perplexity: 7.1481\nEpoch [3/5], Step [1420/3236], Loss: 1.9038, Perplexity: 6.7114\nEpoch [3/5], Step [1430/3236], Loss: 1.9417, Perplexity: 6.9703\nEpoch [3/5], Step [1440/3236], Loss: 1.8255, Perplexity: 6.2059\nEpoch [3/5], Step [1450/3236], Loss: 1.9731, Perplexity: 7.1932\nEpoch [3/5], Step [1460/3236], Loss: 1.9549, Perplexity: 7.0633\nEpoch [3/5], Step [1470/3236], Loss: 1.8139, Perplexity: 6.1346\nEpoch [3/5], Step [1480/3236], Loss: 1.8766, Perplexity: 6.5316\nEpoch [3/5], Step [1490/3236], Loss: 1.8376, Perplexity: 6.2812\nEpoch [3/5], Step [1500/3236], Loss: 1.9379, Perplexity: 6.9444\nEpoch [3/5], Step [1510/3236], Loss: 1.9563, Perplexity: 7.0733\nEpoch [3/5], Step [1520/3236], Loss: 1.9568, Perplexity: 7.0765\nEpoch [3/5], Step [1530/3236], Loss: 1.8845, Perplexity: 6.5828\nEpoch [3/5], Step [1540/3236], Loss: 1.9629, Perplexity: 7.1196\nEpoch [3/5], Step [1550/3236], Loss: 1.8990, Perplexity: 6.6794\nEpoch [3/5], Step [1560/3236], Loss: 1.8602, Perplexity: 6.4248\nEpoch [3/5], Step [1570/3236], Loss: 1.8430, Perplexity: 6.3154\nEpoch [3/5], Step [1580/3236], Loss: 1.8484, Perplexity: 6.3497\nEpoch [3/5], Step [1590/3236], Loss: 1.8620, Perplexity: 6.4365\nEpoch [3/5], Step [1600/3236], Loss: 1.8035, Perplexity: 6.0707\nEpoch [3/5], Step [1610/3236], Loss: 1.9897, Perplexity: 7.3130\nEpoch [3/5], Step [1620/3236], Loss: 1.8861, Perplexity: 6.5938\nEpoch [3/5], Step [1630/3236], Loss: 2.0095, Perplexity: 7.4599\nEpoch [3/5], Step [1640/3236], Loss: 2.0301, Perplexity: 7.6145\nEpoch [3/5], Step [1650/3236], Loss: 2.0022, Perplexity: 7.4053\nEpoch [3/5], Step [1660/3236], Loss: 1.9792, Perplexity: 7.2372\nEpoch [3/5], Step [1670/3236], Loss: 1.9850, Perplexity: 7.2789\nEpoch [3/5], Step [1680/3236], Loss: 2.0599, Perplexity: 7.8449\nEpoch [3/5], Step [1690/3236], Loss: 1.8789, Perplexity: 6.5466\nEpoch [3/5], Step [1700/3236], Loss: 1.9050, Perplexity: 6.7193\nEpoch [3/5], Step [1710/3236], Loss: 1.9882, Perplexity: 7.3021\nEpoch [3/5], Step [1720/3236], Loss: 1.8222, Perplexity: 6.1852\nEpoch [3/5], Step [1730/3236], Loss: 1.9041, Perplexity: 6.7136\nEpoch [3/5], Step [1740/3236], Loss: 1.8743, Perplexity: 6.5161\nEpoch [3/5], Step [1750/3236], Loss: 1.8723, Perplexity: 6.5032\nEpoch [3/5], Step [1760/3236], Loss: 1.8649, Perplexity: 6.4556\nEpoch [3/5], Step [1770/3236], Loss: 1.9310, Perplexity: 6.8965\nEpoch [3/5], Step [1780/3236], Loss: 1.8906, Perplexity: 6.6232\nEpoch [3/5], Step [1790/3236], Loss: 1.9865, Perplexity: 7.2899\nEpoch [3/5], Step [1800/3236], Loss: 1.9309, Perplexity: 6.8954\nEpoch [3/5], Step [1810/3236], Loss: 1.9041, Perplexity: 6.7132\nEpoch [3/5], Step [1820/3236], Loss: 1.7952, Perplexity: 6.0209\nEpoch [3/5], Step [1830/3236], Loss: 1.8634, Perplexity: 6.4459\nEpoch [3/5], Step [1840/3236], Loss: 2.0057, Perplexity: 7.4313\nEpoch [3/5], Step [1850/3236], Loss: 1.9505, Perplexity: 7.0322\nEpoch [3/5], Step [1860/3236], Loss: 1.9604, Perplexity: 7.1025\nEpoch [3/5], Step [1870/3236], Loss: 1.8952, Perplexity: 6.6536\nEpoch [3/5], Step [1880/3236], Loss: 1.8663, Perplexity: 6.4644\nEpoch [3/5], Step [1890/3236], Loss: 1.9209, Perplexity: 6.8272\nEpoch [3/5], Step [1900/3236], Loss: 1.9050, Perplexity: 6.7194\nEpoch [3/5], Step [1910/3236], Loss: 1.9042, Perplexity: 6.7143\nEpoch [3/5], Step [1920/3236], Loss: 1.9462, Perplexity: 7.0024\nEpoch [3/5], Step [1930/3236], Loss: 1.9050, Perplexity: 6.7194\nEpoch [3/5], Step [1940/3236], Loss: 2.1242, Perplexity: 8.3665\nEpoch [3/5], Step [1950/3236], Loss: 1.8596, Perplexity: 6.4210\nEpoch [3/5], Step [1960/3236], Loss: 2.0212, Perplexity: 7.5476\nEpoch [3/5], Step [1970/3236], Loss: 1.8777, Perplexity: 6.5386\nEpoch [3/5], Step [1980/3236], Loss: 1.8855, Perplexity: 6.5899\nEpoch [3/5], Step [1990/3236], Loss: 1.9763, Perplexity: 7.2157\nEpoch [3/5], Step [2000/3236], Loss: 1.9869, Perplexity: 7.2928\nEpoch [3/5], Step [2010/3236], Loss: 1.9310, Perplexity: 6.8961\nEpoch [3/5], Step [2020/3236], Loss: 1.7836, Perplexity: 5.9510\nEpoch [3/5], Step [2030/3236], Loss: 1.9443, Perplexity: 6.9887\nEpoch [3/5], Step [2040/3236], Loss: 1.9155, Perplexity: 6.7904\nEpoch [3/5], Step [2050/3236], Loss: 1.9408, Perplexity: 6.9644\nEpoch [3/5], Step [2060/3236], Loss: 1.9957, Perplexity: 7.3577\nEpoch [3/5], Step [2070/3236], Loss: 1.9051, Perplexity: 6.7203\nEpoch [3/5], Step [2080/3236], Loss: 1.9551, Perplexity: 7.0648\nEpoch [3/5], Step [2090/3236], Loss: 2.0365, Perplexity: 7.6636\nEpoch [3/5], Step [2100/3236], Loss: 1.8806, Perplexity: 6.5572\nEpoch [3/5], Step [2110/3236], Loss: 1.9230, Perplexity: 6.8417\nEpoch [3/5], Step [2120/3236], Loss: 2.0606, Perplexity: 7.8505\nEpoch [3/5], Step [2130/3236], Loss: 1.9037, Perplexity: 6.7109\nEpoch [3/5], Step [2140/3236], Loss: 2.0427, Perplexity: 7.7113\nEpoch [3/5], Step [2150/3236], Loss: 1.9035, Perplexity: 6.7091\nEpoch [3/5], Step [2160/3236], Loss: 1.8556, Perplexity: 6.3955\nEpoch [3/5], Step [2170/3236], Loss: 1.9166, Perplexity: 6.7980\nEpoch [3/5], Step [2180/3236], Loss: 1.8966, Perplexity: 6.6632\nEpoch [3/5], Step [2190/3236], Loss: 1.8419, Perplexity: 6.3087\nEpoch [3/5], Step [2200/3236], Loss: 1.7923, Perplexity: 6.0030\nEpoch [3/5], Step [2210/3236], Loss: 1.9358, Perplexity: 6.9299\nEpoch [3/5], Step [2220/3236], Loss: 1.9390, Perplexity: 6.9518\nEpoch [3/5], Step [2230/3236], Loss: 2.0149, Perplexity: 7.5003\nEpoch [3/5], Step [2240/3236], Loss: 1.9823, Perplexity: 7.2592\nEpoch [3/5], Step [2250/3236], Loss: 1.9006, Perplexity: 6.6899\nEpoch [3/5], Step [2260/3236], Loss: 1.9318, Perplexity: 6.9021\nEpoch [3/5], Step [2270/3236], Loss: 2.0043, Perplexity: 7.4210\nEpoch [3/5], Step [2280/3236], Loss: 1.9292, Perplexity: 6.8843\nEpoch [3/5], Step [2290/3236], Loss: 1.9271, Perplexity: 6.8698\nEpoch [3/5], Step [2300/3236], Loss: 1.9587, Perplexity: 7.0899\nEpoch [3/5], Step [2310/3236], Loss: 1.9313, Perplexity: 6.8983\nEpoch [3/5], Step [2320/3236], Loss: 1.9889, Perplexity: 7.3075\nEpoch [3/5], Step [2330/3236], Loss: 1.9429, Perplexity: 6.9788\nEpoch [3/5], Step [2340/3236], Loss: 2.0565, Perplexity: 7.8185\nEpoch [3/5], Step [2350/3236], Loss: 2.0397, Perplexity: 7.6880\nEpoch [3/5], Step [2360/3236], Loss: 1.9255, Perplexity: 6.8584\nEpoch [3/5], Step [2370/3236], Loss: 1.9148, Perplexity: 6.7856\nEpoch [3/5], Step [2380/3236], Loss: 1.9302, Perplexity: 6.8906\nEpoch [3/5], Step [2390/3236], Loss: 1.9818, Perplexity: 7.2556\nEpoch [3/5], Step [2400/3236], Loss: 1.8860, Perplexity: 6.5927\nEpoch [3/5], Step [2410/3236], Loss: 1.9506, Perplexity: 7.0330\nEpoch [3/5], Step [2420/3236], Loss: 1.9295, Perplexity: 6.8860\nEpoch [3/5], Step [2430/3236], Loss: 2.0314, Perplexity: 7.6246\nEpoch [3/5], Step [2440/3236], Loss: 2.0026, Perplexity: 7.4081\nEpoch [3/5], Step [2450/3236], Loss: 1.9615, Perplexity: 7.1103\nEpoch [3/5], Step [2460/3236], Loss: 1.8998, Perplexity: 6.6846\nEpoch [3/5], Step [2470/3236], Loss: 1.9039, Perplexity: 6.7122\nEpoch [3/5], Step [2480/3236], Loss: 1.9264, Perplexity: 6.8651\nEpoch [3/5], Step [2490/3236], Loss: 1.9266, Perplexity: 6.8664\nEpoch [3/5], Step [2500/3236], Loss: 1.7643, Perplexity: 5.8376\nEpoch [3/5], Step [2510/3236], Loss: 1.8260, Perplexity: 6.2090\nEpoch [3/5], Step [2520/3236], Loss: 1.8597, Perplexity: 6.4218\nEpoch [3/5], Step [2530/3236], Loss: 1.9201, Perplexity: 6.8216\nEpoch [3/5], Step [2540/3236], Loss: 1.8909, Perplexity: 6.6250\nEpoch [3/5], Step [2550/3236], Loss: 1.8885, Perplexity: 6.6091\nEpoch [3/5], Step [2560/3236], Loss: 1.9671, Perplexity: 7.1497\nEpoch [3/5], Step [2570/3236], Loss: 1.9644, Perplexity: 7.1303\nEpoch [3/5], Step [2580/3236], Loss: 1.9133, Perplexity: 6.7752\nEpoch [3/5], Step [2590/3236], Loss: 1.9487, Perplexity: 7.0196\nEpoch [3/5], Step [2600/3236], Loss: 1.8388, Perplexity: 6.2891\nEpoch [3/5], Step [2610/3236], Loss: 1.8924, Perplexity: 6.6356\nEpoch [3/5], Step [2620/3236], Loss: 2.0150, Perplexity: 7.5008\nEpoch [3/5], Step [2630/3236], Loss: 2.0949, Perplexity: 8.1245\nEpoch [3/5], Step [2640/3236], Loss: 1.9786, Perplexity: 7.2324\nEpoch [3/5], Step [2650/3236], Loss: 1.8646, Perplexity: 6.4535\nEpoch [3/5], Step [2660/3236], Loss: 1.9348, Perplexity: 6.9225\nEpoch [3/5], Step [2670/3236], Loss: 2.0176, Perplexity: 7.5200\nEpoch [3/5], Step [2680/3236], Loss: 1.9840, Perplexity: 7.2715\nEpoch [3/5], Step [2690/3236], Loss: 1.9400, Perplexity: 6.9588\nEpoch [3/5], Step [2700/3236], Loss: 1.9234, Perplexity: 6.8439\nEpoch [3/5], Step [2710/3236], Loss: 1.8763, Perplexity: 6.5292\nEpoch [3/5], Step [2720/3236], Loss: 1.8720, Perplexity: 6.5016\nEpoch [3/5], Step [2730/3236], Loss: 1.8691, Perplexity: 6.4825\nEpoch [3/5], Step [2740/3236], Loss: 1.8916, Perplexity: 6.6302\nEpoch [3/5], Step [2750/3236], Loss: 2.0310, Perplexity: 7.6219\nEpoch [3/5], Step [2760/3236], Loss: 1.8838, Perplexity: 6.5783\nEpoch [3/5], Step [2770/3236], Loss: 1.9403, Perplexity: 6.9606\nEpoch [3/5], Step [2780/3236], Loss: 2.0402, Perplexity: 7.6919\nEpoch [3/5], Step [2790/3236], Loss: 1.9586, Perplexity: 7.0891\nEpoch [3/5], Step [2800/3236], Loss: 1.8304, Perplexity: 6.2363\nEpoch [3/5], Step [2810/3236], Loss: 1.9305, Perplexity: 6.8930\nEpoch [3/5], Step [2820/3236], Loss: 1.9483, Perplexity: 7.0170\nEpoch [3/5], Step [2830/3236], Loss: 1.8712, Perplexity: 6.4961\nEpoch [3/5], Step [2840/3236], Loss: 1.9297, Perplexity: 6.8872\nEpoch [3/5], Step [2850/3236], Loss: 1.9326, Perplexity: 6.9077\nEpoch [3/5], Step [2860/3236], Loss: 1.8760, Perplexity: 6.5275\nEpoch [3/5], Step [2870/3236], Loss: 1.8274, Perplexity: 6.2176\nEpoch [3/5], Step [2880/3236], Loss: 1.9875, Perplexity: 7.2971\nEpoch [3/5], Step [2890/3236], Loss: 1.8485, Perplexity: 6.3506\nEpoch [3/5], Step [2900/3236], Loss: 1.7862, Perplexity: 5.9670\nEpoch [3/5], Step [2910/3236], Loss: 2.0022, Perplexity: 7.4050\nEpoch [3/5], Step [2920/3236], Loss: 2.0668, Perplexity: 7.8992\nEpoch [3/5], Step [2930/3236], Loss: 1.9493, Perplexity: 7.0239\nEpoch [3/5], Step [2940/3236], Loss: 1.9252, Perplexity: 6.8566\nEpoch [3/5], Step [2950/3236], Loss: 1.9202, Perplexity: 6.8226\nEpoch [3/5], Step [2960/3236], Loss: 2.0569, Perplexity: 7.8215\nEpoch [3/5], Step [2970/3236], Loss: 1.8401, Perplexity: 6.2973\nEpoch [3/5], Step [2980/3236], Loss: 1.9363, Perplexity: 6.9330\nEpoch [3/5], Step [2990/3236], Loss: 1.9984, Perplexity: 7.3776\nEpoch [3/5], Step [3000/3236], Loss: 1.9663, Perplexity: 7.1443\nEpoch [3/5], Step [3010/3236], Loss: 2.0551, Perplexity: 7.8078\nEpoch [3/5], Step [3020/3236], Loss: 1.9217, Perplexity: 6.8324\nEpoch [3/5], Step [3030/3236], Loss: 1.9282, Perplexity: 6.8771\nEpoch [3/5], Step [3040/3236], Loss: 1.8792, Perplexity: 6.5482\nEpoch [3/5], Step [3050/3236], Loss: 1.8999, Perplexity: 6.6851\nEpoch [3/5], Step [3060/3236], Loss: 2.0759, Perplexity: 7.9718\nEpoch [3/5], Step [3070/3236], Loss: 1.8683, Perplexity: 6.4771\nEpoch [3/5], Step [3080/3236], Loss: 2.0163, Perplexity: 7.5105\nEpoch [3/5], Step [3090/3236], Loss: 1.9251, Perplexity: 6.8557\nEpoch [3/5], Step [3100/3236], Loss: 1.9502, Perplexity: 7.0300\nEpoch [3/5], Step [3110/3236], Loss: 1.9217, Perplexity: 6.8326\nEpoch [3/5], Step [3120/3236], Loss: 2.0317, Perplexity: 7.6269\nEpoch [3/5], Step [3130/3236], Loss: 1.9277, Perplexity: 6.8736\nEpoch [3/5], Step [3140/3236], Loss: 2.0086, Perplexity: 7.4528\nEpoch [3/5], Step [3150/3236], Loss: 1.9058, Perplexity: 6.7250\nEpoch [3/5], Step [3160/3236], Loss: 1.9670, Perplexity: 7.1493\nEpoch [3/5], Step [3170/3236], Loss: 1.7163, Perplexity: 5.5638\nEpoch [3/5], Step [3180/3236], Loss: 1.9085, Perplexity: 6.7427\nEpoch [3/5], Step [3190/3236], Loss: 1.9179, Perplexity: 6.8064\nEpoch [3/5], Step [3200/3236], Loss: 1.9254, Perplexity: 6.8581\nEpoch [3/5], Step [3210/3236], Loss: 1.9333, Perplexity: 6.9121\nEpoch [3/5], Step [3220/3236], Loss: 1.9105, Perplexity: 6.7565\nEpoch [3/5], Step [3230/3236], Loss: 1.9324, Perplexity: 6.9060\nEpoch [4/5], Step [0/3236], Loss: 1.7835, Perplexity: 5.9505\nEpoch [4/5], Step [10/3236], Loss: 1.8058, Perplexity: 6.0847\nEpoch [4/5], Step [20/3236], Loss: 1.7245, Perplexity: 5.6097\nEpoch [4/5], Step [30/3236], Loss: 1.8418, Perplexity: 6.3082\nEpoch [4/5], Step [40/3236], Loss: 1.8361, Perplexity: 6.2723\nEpoch [4/5], Step [50/3236], Loss: 1.7012, Perplexity: 5.4803\nEpoch [4/5], Step [60/3236], Loss: 1.7541, Perplexity: 5.7780\nEpoch [4/5], Step [70/3236], Loss: 1.9050, Perplexity: 6.7196\nEpoch [4/5], Step [80/3236], Loss: 1.8221, Perplexity: 6.1851\nEpoch [4/5], Step [90/3236], Loss: 1.7050, Perplexity: 5.5013\nEpoch [4/5], Step [100/3236], Loss: 1.8208, Perplexity: 6.1770\nEpoch [4/5], Step [110/3236], Loss: 1.7625, Perplexity: 5.8269\nEpoch [4/5], Step [120/3236], Loss: 1.7494, Perplexity: 5.7511\nEpoch [4/5], Step [130/3236], Loss: 1.8788, Perplexity: 6.5456\nEpoch [4/5], Step [140/3236], Loss: 1.8255, Perplexity: 6.2060\nEpoch [4/5], Step [150/3236], Loss: 1.8394, Perplexity: 6.2925\nEpoch [4/5], Step [160/3236], Loss: 1.7015, Perplexity: 5.4823\nEpoch [4/5], Step [170/3236], Loss: 1.7228, Perplexity: 5.6000\nEpoch [4/5], Step [180/3236], Loss: 1.8157, Perplexity: 6.1454\nEpoch [4/5], Step [190/3236], Loss: 1.7446, Perplexity: 5.7237\nEpoch [4/5], Step [200/3236], Loss: 1.8614, Perplexity: 6.4325\nEpoch [4/5], Step [210/3236], Loss: 1.8110, Perplexity: 6.1163\nEpoch [4/5], Step [220/3236], Loss: 1.8030, Perplexity: 6.0679\nEpoch [4/5], Step [230/3236], Loss: 1.7866, Perplexity: 5.9691\nEpoch [4/5], Step [240/3236], Loss: 1.8147, Perplexity: 6.1395\nEpoch [4/5], Step [250/3236], Loss: 1.8305, Perplexity: 6.2369\nEpoch [4/5], Step [260/3236], Loss: 1.8648, Perplexity: 6.4545\nEpoch [4/5], Step [270/3236], Loss: 1.9734, Perplexity: 7.1954\nEpoch [4/5], Step [280/3236], Loss: 1.8572, Perplexity: 6.4055\nEpoch [4/5], Step [290/3236], Loss: 1.8823, Perplexity: 6.5686\nEpoch [4/5], Step [300/3236], Loss: 1.8434, Perplexity: 6.3183\nEpoch [4/5], Step [310/3236], Loss: 1.8572, Perplexity: 6.4059\nEpoch [4/5], Step [320/3236], Loss: 1.8224, Perplexity: 6.1869\nEpoch [4/5], Step [330/3236], Loss: 1.8745, Perplexity: 6.5173\nEpoch [4/5], Step [340/3236], Loss: 1.7978, Perplexity: 6.0365\nEpoch [4/5], Step [350/3236], Loss: 1.7566, Perplexity: 5.7928\nEpoch [4/5], Step [360/3236], Loss: 1.7404, Perplexity: 5.6996\nEpoch [4/5], Step [370/3236], Loss: 1.9002, Perplexity: 6.6875\nEpoch [4/5], Step [380/3236], Loss: 1.8875, Perplexity: 6.6025\nEpoch [4/5], Step [390/3236], Loss: 1.9014, Perplexity: 6.6950\nEpoch [4/5], Step [400/3236], Loss: 1.8467, Perplexity: 6.3387\nEpoch [4/5], Step [410/3236], Loss: 1.7655, Perplexity: 5.8443\nEpoch [4/5], Step [420/3236], Loss: 1.7239, Perplexity: 5.6063\nEpoch [4/5], Step [430/3236], Loss: 1.7722, Perplexity: 5.8838\nEpoch [4/5], Step [440/3236], Loss: 1.8545, Perplexity: 6.3884\nEpoch [4/5], Step [450/3236], Loss: 1.8580, Perplexity: 6.4108\nEpoch [4/5], Step [460/3236], Loss: 1.7227, Perplexity: 5.5996\nEpoch [4/5], Step [470/3236], Loss: 1.7861, Perplexity: 5.9664\nEpoch [4/5], Step [480/3236], Loss: 1.8230, Perplexity: 6.1905\nEpoch [4/5], Step [490/3236], Loss: 1.7580, Perplexity: 5.8007\nEpoch [4/5], Step [500/3236], Loss: 1.7786, Perplexity: 5.9216\nEpoch [4/5], Step [510/3236], Loss: 1.8281, Perplexity: 6.2223\nEpoch [4/5], Step [520/3236], Loss: 1.8624, Perplexity: 6.4393\nEpoch [4/5], Step [530/3236], Loss: 1.8704, Perplexity: 6.4910\nEpoch [4/5], Step [540/3236], Loss: 1.8643, Perplexity: 6.4513\nEpoch [4/5], Step [550/3236], Loss: 1.8769, Perplexity: 6.5331\nEpoch [4/5], Step [560/3236], Loss: 1.8206, Perplexity: 6.1754\nEpoch [4/5], Step [570/3236], Loss: 1.7838, Perplexity: 5.9527\nEpoch [4/5], Step [580/3236], Loss: 1.8140, Perplexity: 6.1352\nEpoch [4/5], Step [590/3236], Loss: 1.9324, Perplexity: 6.9063\nEpoch [4/5], Step [600/3236], Loss: 1.8388, Perplexity: 6.2889\nEpoch [4/5], Step [610/3236], Loss: 1.7485, Perplexity: 5.7461\nEpoch [4/5], Step [620/3236], Loss: 1.8187, Perplexity: 6.1640\nEpoch [4/5], Step [630/3236], Loss: 1.8349, Perplexity: 6.2647\nEpoch [4/5], Step [640/3236], Loss: 1.8721, Perplexity: 6.5019\nEpoch [4/5], Step [650/3236], Loss: 1.7676, Perplexity: 5.8570\nEpoch [4/5], Step [660/3236], Loss: 1.7545, Perplexity: 5.7803\nEpoch [4/5], Step [670/3236], Loss: 1.8609, Perplexity: 6.4293\nEpoch [4/5], Step [680/3236], Loss: 1.8206, Perplexity: 6.1757\nEpoch [4/5], Step [690/3236], Loss: 1.8072, Perplexity: 6.0933\nEpoch [4/5], Step [700/3236], Loss: 1.7244, Perplexity: 5.6091\nEpoch [4/5], Step [710/3236], Loss: 1.6925, Perplexity: 5.4329\nEpoch [4/5], Step [720/3236], Loss: 1.7854, Perplexity: 5.9623\nEpoch [4/5], Step [730/3236], Loss: 1.9098, Perplexity: 6.7518\nEpoch [4/5], Step [740/3236], Loss: 1.6965, Perplexity: 5.4548\nEpoch [4/5], Step [750/3236], Loss: 1.9208, Perplexity: 6.8263\nEpoch [4/5], Step [760/3236], Loss: 1.9154, Perplexity: 6.7899\nEpoch [4/5], Step [770/3236], Loss: 1.7175, Perplexity: 5.5707\nEpoch [4/5], Step [780/3236], Loss: 1.8452, Perplexity: 6.3292\nEpoch [4/5], Step [790/3236], Loss: 1.7948, Perplexity: 6.0182\nEpoch [4/5], Step [800/3236], Loss: 1.8134, Perplexity: 6.1314\nEpoch [4/5], Step [810/3236], Loss: 1.8367, Perplexity: 6.2760\nEpoch [4/5], Step [820/3236], Loss: 1.9378, Perplexity: 6.9432\nEpoch [4/5], Step [830/3236], Loss: 1.7656, Perplexity: 5.8450\nEpoch [4/5], Step [840/3236], Loss: 1.9439, Perplexity: 6.9857\nEpoch [4/5], Step [850/3236], Loss: 1.8260, Perplexity: 6.2087\nEpoch [4/5], Step [860/3236], Loss: 1.8763, Perplexity: 6.5291\nEpoch [4/5], Step [870/3236], Loss: 1.8817, Perplexity: 6.5645\nEpoch [4/5], Step [880/3236], Loss: 1.7547, Perplexity: 5.7815\nEpoch [4/5], Step [890/3236], Loss: 1.8102, Perplexity: 6.1115\nEpoch [4/5], Step [900/3236], Loss: 1.8624, Perplexity: 6.4391\nEpoch [4/5], Step [910/3236], Loss: 1.9463, Perplexity: 7.0029\nEpoch [4/5], Step [920/3236], Loss: 1.8420, Perplexity: 6.3089\nEpoch [4/5], Step [930/3236], Loss: 1.8923, Perplexity: 6.6349\nEpoch [4/5], Step [940/3236], Loss: 1.8997, Perplexity: 6.6838\nEpoch [4/5], Step [950/3236], Loss: 1.8089, Perplexity: 6.1037\nEpoch [4/5], Step [960/3236], Loss: 1.8894, Perplexity: 6.6152\nEpoch [4/5], Step [970/3236], Loss: 1.7274, Perplexity: 5.6258\nEpoch [4/5], Step [980/3236], Loss: 1.8450, Perplexity: 6.3282\nEpoch [4/5], Step [990/3236], Loss: 1.8970, Perplexity: 6.6658\nEpoch [4/5], Step [1000/3236], Loss: 1.8407, Perplexity: 6.3011\nEpoch [4/5], Step [1010/3236], Loss: 1.8982, Perplexity: 6.6737\nEpoch [4/5], Step [1020/3236], Loss: 1.8510, Perplexity: 6.3662\nEpoch [4/5], Step [1030/3236], Loss: 1.8614, Perplexity: 6.4327\nEpoch [4/5], Step [1040/3236], Loss: 1.8578, Perplexity: 6.4098\nEpoch [4/5], Step [1050/3236], Loss: 1.8084, Perplexity: 6.1005\nEpoch [4/5], Step [1060/3236], Loss: 1.8299, Perplexity: 6.2333\nEpoch [4/5], Step [1070/3236], Loss: 1.7626, Perplexity: 5.8276\nEpoch [4/5], Step [1080/3236], Loss: 1.8819, Perplexity: 6.5661\nEpoch [4/5], Step [1090/3236], Loss: 1.8605, Perplexity: 6.4271\nEpoch [4/5], Step [1100/3236], Loss: 1.8670, Perplexity: 6.4686\nEpoch [4/5], Step [1110/3236], Loss: 1.9188, Perplexity: 6.8128\nEpoch [4/5], Step [1120/3236], Loss: 1.9170, Perplexity: 6.8007\nEpoch [4/5], Step [1130/3236], Loss: 1.8593, Perplexity: 6.4190\nEpoch [4/5], Step [1140/3236], Loss: 1.9208, Perplexity: 6.8263\nEpoch [4/5], Step [1150/3236], Loss: 1.9494, Perplexity: 7.0244\nEpoch [4/5], Step [1160/3236], Loss: 1.8713, Perplexity: 6.4969\nEpoch [4/5], Step [1170/3236], Loss: 1.9033, Perplexity: 6.7078\nEpoch [4/5], Step [1180/3236], Loss: 1.9482, Perplexity: 7.0159\nEpoch [4/5], Step [1190/3236], Loss: 1.8736, Perplexity: 6.5120\nEpoch [4/5], Step [1200/3236], Loss: 2.0255, Perplexity: 7.5797\nEpoch [4/5], Step [1210/3236], Loss: 1.8404, Perplexity: 6.2991\nEpoch [4/5], Step [1220/3236], Loss: 1.8242, Perplexity: 6.1981\nEpoch [4/5], Step [1230/3236], Loss: 1.8400, Perplexity: 6.2966\nEpoch [4/5], Step [1240/3236], Loss: 1.8927, Perplexity: 6.6373\nEpoch [4/5], Step [1250/3236], Loss: 1.8805, Perplexity: 6.5570\nEpoch [4/5], Step [1260/3236], Loss: 1.7988, Perplexity: 6.0425\nEpoch [4/5], Step [1270/3236], Loss: 1.8294, Perplexity: 6.2303\nEpoch [4/5], Step [1280/3236], Loss: 1.8663, Perplexity: 6.4645\nEpoch [4/5], Step [1290/3236], Loss: 1.9279, Perplexity: 6.8751\nEpoch [4/5], Step [1300/3236], Loss: 1.8316, Perplexity: 6.2440\nEpoch [4/5], Step [1310/3236], Loss: 1.7990, Perplexity: 6.0435\nEpoch [4/5], Step [1320/3236], Loss: 1.8974, Perplexity: 6.6688\nEpoch [4/5], Step [1330/3236], Loss: 1.9243, Perplexity: 6.8504\nEpoch [4/5], Step [1340/3236], Loss: 1.8056, Perplexity: 6.0837\nEpoch [4/5], Step [1350/3236], Loss: 1.7305, Perplexity: 5.6437\nEpoch [4/5], Step [1360/3236], Loss: 1.9557, Perplexity: 7.0686\nEpoch [4/5], Step [1370/3236], Loss: 1.8096, Perplexity: 6.1082\nEpoch [4/5], Step [1380/3236], Loss: 1.8370, Perplexity: 6.2777\nEpoch [4/5], Step [1390/3236], Loss: 1.7297, Perplexity: 5.6391\nEpoch [4/5], Step [1400/3236], Loss: 1.8649, Perplexity: 6.4553\nEpoch [4/5], Step [1410/3236], Loss: 1.7892, Perplexity: 5.9850\nEpoch [4/5], Step [1420/3236], Loss: 1.7708, Perplexity: 5.8753\nEpoch [4/5], Step [1430/3236], Loss: 1.8581, Perplexity: 6.4116\nEpoch [4/5], Step [1440/3236], Loss: 1.9667, Perplexity: 7.1471\nEpoch [4/5], Step [1450/3236], Loss: 1.9601, Perplexity: 7.0999\nEpoch [4/5], Step [1460/3236], Loss: 1.8460, Perplexity: 6.3343\nEpoch [4/5], Step [1470/3236], Loss: 1.8299, Perplexity: 6.2331\nEpoch [4/5], Step [1480/3236], Loss: 1.8969, Perplexity: 6.6655\nEpoch [4/5], Step [1490/3236], Loss: 1.9051, Perplexity: 6.7199\nEpoch [4/5], Step [1500/3236], Loss: 1.8071, Perplexity: 6.0929\nEpoch [4/5], Step [1510/3236], Loss: 1.7932, Perplexity: 6.0088\nEpoch [4/5], Step [1520/3236], Loss: 1.8376, Perplexity: 6.2815\nEpoch [4/5], Step [1530/3236], Loss: 1.8830, Perplexity: 6.5735\nEpoch [4/5], Step [1540/3236], Loss: 1.7511, Perplexity: 5.7607\nEpoch [4/5], Step [1550/3236], Loss: 1.8725, Perplexity: 6.5047\nEpoch [4/5], Step [1560/3236], Loss: 1.8096, Perplexity: 6.1083\nEpoch [4/5], Step [1570/3236], Loss: 1.8743, Perplexity: 6.5161\nEpoch [4/5], Step [1580/3236], Loss: 1.8992, Perplexity: 6.6806\nEpoch [4/5], Step [1590/3236], Loss: 1.7593, Perplexity: 5.8086\nEpoch [4/5], Step [1600/3236], Loss: 1.8427, Perplexity: 6.3133\nEpoch [4/5], Step [1610/3236], Loss: 1.8338, Perplexity: 6.2576\nEpoch [4/5], Step [1620/3236], Loss: 1.9682, Perplexity: 7.1579\nEpoch [4/5], Step [1630/3236], Loss: 1.8631, Perplexity: 6.4438\nEpoch [4/5], Step [1640/3236], Loss: 1.9263, Perplexity: 6.8641\nEpoch [4/5], Step [1650/3236], Loss: 1.7879, Perplexity: 5.9768\nEpoch [4/5], Step [1660/3236], Loss: 1.9817, Perplexity: 7.2548\nEpoch [4/5], Step [1670/3236], Loss: 1.8364, Perplexity: 6.2740\nEpoch [4/5], Step [1680/3236], Loss: 1.8110, Perplexity: 6.1168\nEpoch [4/5], Step [1690/3236], Loss: 1.9287, Perplexity: 6.8803\nEpoch [4/5], Step [1700/3236], Loss: 1.8165, Perplexity: 6.1502\nEpoch [4/5], Step [1710/3236], Loss: 1.8573, Perplexity: 6.4065\nEpoch [4/5], Step [1720/3236], Loss: 1.9155, Perplexity: 6.7906\nEpoch [4/5], Step [1730/3236], Loss: 1.8217, Perplexity: 6.1821\nEpoch [4/5], Step [1740/3236], Loss: 2.0131, Perplexity: 7.4866\nEpoch [4/5], Step [1750/3236], Loss: 1.9195, Perplexity: 6.8178\nEpoch [4/5], Step [1760/3236], Loss: 1.8526, Perplexity: 6.3765\nEpoch [4/5], Step [1770/3236], Loss: 1.8964, Perplexity: 6.6619\nEpoch [4/5], Step [1780/3236], Loss: 1.8478, Perplexity: 6.3458\nEpoch [4/5], Step [1790/3236], Loss: 1.8269, Perplexity: 6.2146\nEpoch [4/5], Step [1800/3236], Loss: 1.7812, Perplexity: 5.9367\nEpoch [4/5], Step [1810/3236], Loss: 1.9128, Perplexity: 6.7717\nEpoch [4/5], Step [1820/3236], Loss: 1.8853, Perplexity: 6.5880\nEpoch [4/5], Step [1830/3236], Loss: 1.8582, Perplexity: 6.4119\nEpoch [4/5], Step [1840/3236], Loss: 1.8266, Perplexity: 6.2130\nEpoch [4/5], Step [1850/3236], Loss: 1.8905, Perplexity: 6.6224\nEpoch [4/5], Step [1860/3236], Loss: 1.8932, Perplexity: 6.6403\nEpoch [4/5], Step [1870/3236], Loss: 1.7823, Perplexity: 5.9436\nEpoch [4/5], Step [1880/3236], Loss: 2.0284, Perplexity: 7.6020\nEpoch [4/5], Step [1890/3236], Loss: 1.8319, Perplexity: 6.2457\nEpoch [4/5], Step [1900/3236], Loss: 1.9296, Perplexity: 6.8866\nEpoch [4/5], Step [1910/3236], Loss: 1.8803, Perplexity: 6.5553\nEpoch [4/5], Step [1920/3236], Loss: 1.8358, Perplexity: 6.2702\nEpoch [4/5], Step [1930/3236], Loss: 1.7915, Perplexity: 5.9983\nEpoch [4/5], Step [1940/3236], Loss: 1.8363, Perplexity: 6.2735\nEpoch [4/5], Step [1950/3236], Loss: 1.8706, Perplexity: 6.4919\nEpoch [4/5], Step [1960/3236], Loss: 1.7425, Perplexity: 5.7117\nEpoch [4/5], Step [1970/3236], Loss: 1.8986, Perplexity: 6.6767\nEpoch [4/5], Step [1980/3236], Loss: 1.9202, Perplexity: 6.8223\nEpoch [4/5], Step [1990/3236], Loss: 1.7931, Perplexity: 6.0081\nEpoch [4/5], Step [2000/3236], Loss: 1.8667, Perplexity: 6.4671\nEpoch [4/5], Step [2010/3236], Loss: 1.7372, Perplexity: 5.6812\nEpoch [4/5], Step [2020/3236], Loss: 1.8601, Perplexity: 6.4243\nEpoch [4/5], Step [2030/3236], Loss: 1.8538, Perplexity: 6.3839\nEpoch [4/5], Step [2040/3236], Loss: 1.9203, Perplexity: 6.8231\nEpoch [4/5], Step [2050/3236], Loss: 1.8871, Perplexity: 6.6002\nEpoch [4/5], Step [2060/3236], Loss: 1.8806, Perplexity: 6.5572\nEpoch [4/5], Step [2070/3236], Loss: 1.9326, Perplexity: 6.9078\nEpoch [4/5], Step [2080/3236], Loss: 1.8824, Perplexity: 6.5696\nEpoch [4/5], Step [2090/3236], Loss: 1.8894, Perplexity: 6.6157\nEpoch [4/5], Step [2100/3236], Loss: 1.8777, Perplexity: 6.5384\nEpoch [4/5], Step [2110/3236], Loss: 1.8440, Perplexity: 6.3217\nEpoch [4/5], Step [2120/3236], Loss: 1.8516, Perplexity: 6.3701\nEpoch [4/5], Step [2130/3236], Loss: 1.9457, Perplexity: 6.9989\nEpoch [4/5], Step [2140/3236], Loss: 1.8859, Perplexity: 6.5925\nEpoch [4/5], Step [2150/3236], Loss: 1.8478, Perplexity: 6.3458\nEpoch [4/5], Step [2160/3236], Loss: 1.9092, Perplexity: 6.7475\nEpoch [4/5], Step [2170/3236], Loss: 1.8347, Perplexity: 6.2631\nEpoch [4/5], Step [2180/3236], Loss: 1.9573, Perplexity: 7.0800\nEpoch [4/5], Step [2190/3236], Loss: 1.8312, Perplexity: 6.2411\nEpoch [4/5], Step [2200/3236], Loss: 1.8061, Perplexity: 6.0869\nEpoch [4/5], Step [2210/3236], Loss: 1.8382, Perplexity: 6.2849\nEpoch [4/5], Step [2220/3236], Loss: 1.9374, Perplexity: 6.9406\nEpoch [4/5], Step [2230/3236], Loss: 2.0246, Perplexity: 7.5729\nEpoch [4/5], Step [2240/3236], Loss: 1.9079, Perplexity: 6.7392\nEpoch [4/5], Step [2250/3236], Loss: 1.8783, Perplexity: 6.5423\nEpoch [4/5], Step [2260/3236], Loss: 1.8960, Perplexity: 6.6589\nEpoch [4/5], Step [2270/3236], Loss: 1.7782, Perplexity: 5.9192\nEpoch [4/5], Step [2280/3236], Loss: 1.9335, Perplexity: 6.9136\nEpoch [4/5], Step [2290/3236], Loss: 1.8525, Perplexity: 6.3758\nEpoch [4/5], Step [2300/3236], Loss: 1.8872, Perplexity: 6.6006\nEpoch [4/5], Step [2310/3236], Loss: 1.8683, Perplexity: 6.4773\nEpoch [4/5], Step [2320/3236], Loss: 1.9321, Perplexity: 6.9038\nEpoch [4/5], Step [2330/3236], Loss: 1.9129, Perplexity: 6.7728\nEpoch [4/5], Step [2340/3236], Loss: 1.7627, Perplexity: 5.8281\nEpoch [4/5], Step [2350/3236], Loss: 1.8494, Perplexity: 6.3557\nEpoch [4/5], Step [2360/3236], Loss: 1.8950, Perplexity: 6.6525\nEpoch [4/5], Step [2370/3236], Loss: 1.8836, Perplexity: 6.5768\nEpoch [4/5], Step [2380/3236], Loss: 1.7908, Perplexity: 5.9940\nEpoch [4/5], Step [2390/3236], Loss: 1.9359, Perplexity: 6.9306\nEpoch [4/5], Step [2400/3236], Loss: 1.9090, Perplexity: 6.7467\nEpoch [4/5], Step [2410/3236], Loss: 1.8740, Perplexity: 6.5143\nEpoch [4/5], Step [2420/3236], Loss: 1.7945, Perplexity: 6.0166\nEpoch [4/5], Step [2430/3236], Loss: 1.7271, Perplexity: 5.6242\nEpoch [4/5], Step [2440/3236], Loss: 1.8272, Perplexity: 6.2164\nEpoch [4/5], Step [2450/3236], Loss: 1.8811, Perplexity: 6.5608\nEpoch [4/5], Step [2460/3236], Loss: 2.0033, Perplexity: 7.4134\nEpoch [4/5], Step [2470/3236], Loss: 1.9217, Perplexity: 6.8326\nEpoch [4/5], Step [2480/3236], Loss: 1.9046, Perplexity: 6.7167\nEpoch [4/5], Step [2490/3236], Loss: 1.9386, Perplexity: 6.9490\nEpoch [4/5], Step [2500/3236], Loss: 1.8979, Perplexity: 6.6717\nEpoch [4/5], Step [2510/3236], Loss: 1.9026, Perplexity: 6.7035\nEpoch [4/5], Step [2520/3236], Loss: 1.7564, Perplexity: 5.7918\nEpoch [4/5], Step [2530/3236], Loss: 1.9532, Perplexity: 7.0515\nEpoch [4/5], Step [2540/3236], Loss: 1.8517, Perplexity: 6.3705\nEpoch [4/5], Step [2550/3236], Loss: 1.8936, Perplexity: 6.6432\nEpoch [4/5], Step [2560/3236], Loss: 1.7530, Perplexity: 5.7716\nEpoch [4/5], Step [2570/3236], Loss: 1.9689, Perplexity: 7.1626\nEpoch [4/5], Step [2580/3236], Loss: 1.8117, Perplexity: 6.1207\nEpoch [4/5], Step [2590/3236], Loss: 1.7893, Perplexity: 5.9851\nEpoch [4/5], Step [2600/3236], Loss: 1.8915, Perplexity: 6.6293\nEpoch [4/5], Step [2610/3236], Loss: 1.7983, Perplexity: 6.0392\nEpoch [4/5], Step [2620/3236], Loss: 1.8809, Perplexity: 6.5596\nEpoch [4/5], Step [2630/3236], Loss: 1.9039, Perplexity: 6.7121\nEpoch [4/5], Step [2640/3236], Loss: 1.8839, Perplexity: 6.5789\nEpoch [4/5], Step [2650/3236], Loss: 1.8047, Perplexity: 6.0783\nEpoch [4/5], Step [2660/3236], Loss: 1.8768, Perplexity: 6.5328\nEpoch [4/5], Step [2670/3236], Loss: 1.7796, Perplexity: 5.9272\nEpoch [4/5], Step [2680/3236], Loss: 1.9297, Perplexity: 6.8877\nEpoch [4/5], Step [2690/3236], Loss: 1.8793, Perplexity: 6.5492\nEpoch [4/5], Step [2700/3236], Loss: 1.7919, Perplexity: 6.0011\nEpoch [4/5], Step [2710/3236], Loss: 1.8646, Perplexity: 6.4531\nEpoch [4/5], Step [2720/3236], Loss: 1.8959, Perplexity: 6.6584\nEpoch [4/5], Step [2730/3236], Loss: 1.7791, Perplexity: 5.9242\nEpoch [4/5], Step [2740/3236], Loss: 1.8975, Perplexity: 6.6690\nEpoch [4/5], Step [2750/3236], Loss: 1.7860, Perplexity: 5.9658\nEpoch [4/5], Step [2760/3236], Loss: 1.8009, Perplexity: 6.0553\nEpoch [4/5], Step [2770/3236], Loss: 1.9042, Perplexity: 6.7143\nEpoch [4/5], Step [2780/3236], Loss: 1.9037, Perplexity: 6.7105\nEpoch [4/5], Step [2790/3236], Loss: 1.8943, Perplexity: 6.6476\nEpoch [4/5], Step [2800/3236], Loss: 1.8937, Perplexity: 6.6437\nEpoch [4/5], Step [2810/3236], Loss: 1.7287, Perplexity: 5.6334\nEpoch [4/5], Step [2820/3236], Loss: 1.8415, Perplexity: 6.3061\nEpoch [4/5], Step [2830/3236], Loss: 1.9105, Perplexity: 6.7567\nEpoch [4/5], Step [2840/3236], Loss: 1.8735, Perplexity: 6.5113\nEpoch [4/5], Step [2850/3236], Loss: 1.8678, Perplexity: 6.4743\nEpoch [4/5], Step [2860/3236], Loss: 1.8922, Perplexity: 6.6338\nEpoch [4/5], Step [2870/3236], Loss: 1.9253, Perplexity: 6.8575\nEpoch [4/5], Step [2880/3236], Loss: 1.7790, Perplexity: 5.9238\nEpoch [4/5], Step [2890/3236], Loss: 1.7673, Perplexity: 5.8553\nEpoch [4/5], Step [2900/3236], Loss: 1.8828, Perplexity: 6.5721\nEpoch [4/5], Step [2910/3236], Loss: 1.8638, Perplexity: 6.4484\nEpoch [4/5], Step [2920/3236], Loss: 1.8329, Perplexity: 6.2522\nEpoch [4/5], Step [2930/3236], Loss: 1.8629, Perplexity: 6.4422\nEpoch [4/5], Step [2940/3236], Loss: 1.9597, Perplexity: 7.0974\nEpoch [4/5], Step [2950/3236], Loss: 1.9048, Perplexity: 6.7181\nEpoch [4/5], Step [2960/3236], Loss: 1.9613, Perplexity: 7.1086\nEpoch [4/5], Step [2970/3236], Loss: 1.9616, Perplexity: 7.1109\nEpoch [4/5], Step [2980/3236], Loss: 1.8796, Perplexity: 6.5508\nEpoch [4/5], Step [2990/3236], Loss: 1.8396, Perplexity: 6.2941\nEpoch [4/5], Step [3000/3236], Loss: 1.9272, Perplexity: 6.8704\nEpoch [4/5], Step [3010/3236], Loss: 1.7567, Perplexity: 5.7933\nEpoch [4/5], Step [3020/3236], Loss: 1.9073, Perplexity: 6.7349\nEpoch [4/5], Step [3030/3236], Loss: 1.9188, Perplexity: 6.8129\nEpoch [4/5], Step [3040/3236], Loss: 1.8974, Perplexity: 6.6685\nEpoch [4/5], Step [3050/3236], Loss: 1.8781, Perplexity: 6.5409\nEpoch [4/5], Step [3060/3236], Loss: 1.7538, Perplexity: 5.7765\nEpoch [4/5], Step [3070/3236], Loss: 1.9488, Perplexity: 7.0200\nEpoch [4/5], Step [3080/3236], Loss: 1.7871, Perplexity: 5.9721\nEpoch [4/5], Step [3090/3236], Loss: 1.7831, Perplexity: 5.9481\nEpoch [4/5], Step [3100/3236], Loss: 1.8575, Perplexity: 6.4074\nEpoch [4/5], Step [3110/3236], Loss: 1.8624, Perplexity: 6.4392\nEpoch [4/5], Step [3120/3236], Loss: 1.8206, Perplexity: 6.1753\nEpoch [4/5], Step [3130/3236], Loss: 1.9695, Perplexity: 7.1670\nEpoch [4/5], Step [3140/3236], Loss: 1.8111, Perplexity: 6.1171\nEpoch [4/5], Step [3150/3236], Loss: 1.8021, Perplexity: 6.0621\nEpoch [4/5], Step [3160/3236], Loss: 1.8505, Perplexity: 6.3630\nEpoch [4/5], Step [3170/3236], Loss: 1.8505, Perplexity: 6.3628\nEpoch [4/5], Step [3180/3236], Loss: 1.9476, Perplexity: 7.0117\nEpoch [4/5], Step [3190/3236], Loss: 1.8747, Perplexity: 6.5186\nEpoch [4/5], Step [3200/3236], Loss: 1.8475, Perplexity: 6.3442\nEpoch [4/5], Step [3210/3236], Loss: 1.9986, Perplexity: 7.3788\nEpoch [4/5], Step [3220/3236], Loss: 1.8853, Perplexity: 6.5881\nEpoch [4/5], Step [3230/3236], Loss: 1.8835, Perplexity: 6.5762\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T12:41:12.251916Z","iopub.execute_input":"2025-08-24T12:41:12.252420Z","iopub.status.idle":"2025-08-24T12:41:12.257254Z","shell.execute_reply.started":"2025-08-24T12:41:12.252387Z","shell.execute_reply":"2025-08-24T12:41:12.256442Z"}},"outputs":[{"name":"stdout","text":"2025-08-24 12:41:12.253810\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}